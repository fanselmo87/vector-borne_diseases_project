{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3f6f901",
   "metadata": {},
   "source": [
    "## Carregando os dados e checando se está ok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1f300f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Ano</th>\n",
       "      <th>Mes</th>\n",
       "      <th>Dengue</th>\n",
       "      <th>Zika</th>\n",
       "      <th>Chik</th>\n",
       "      <th>LeishVis</th>\n",
       "      <th>LeishTeg</th>\n",
       "      <th>LeishT</th>\n",
       "      <th>Precipt</th>\n",
       "      <th>AvgTemp</th>\n",
       "      <th>MaxTemp</th>\n",
       "      <th>MinTemp</th>\n",
       "      <th>AvgHumid</th>\n",
       "      <th>AvgWin</th>\n",
       "      <th>Season</th>\n",
       "      <th>Season2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2014</td>\n",
       "      <td>Janeiro</td>\n",
       "      <td>2901</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15</td>\n",
       "      <td>31</td>\n",
       "      <td>46</td>\n",
       "      <td>138.90</td>\n",
       "      <td>25.12</td>\n",
       "      <td>25.86</td>\n",
       "      <td>24.41</td>\n",
       "      <td>64.82</td>\n",
       "      <td>1.56</td>\n",
       "      <td>Summer</td>\n",
       "      <td>Wet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2014</td>\n",
       "      <td>Fevereiro</td>\n",
       "      <td>10196</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20</td>\n",
       "      <td>32</td>\n",
       "      <td>52</td>\n",
       "      <td>139.67</td>\n",
       "      <td>25.79</td>\n",
       "      <td>26.49</td>\n",
       "      <td>25.10</td>\n",
       "      <td>58.32</td>\n",
       "      <td>1.74</td>\n",
       "      <td>Summer</td>\n",
       "      <td>Wet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2014</td>\n",
       "      <td>Marco</td>\n",
       "      <td>29647</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18</td>\n",
       "      <td>29</td>\n",
       "      <td>47</td>\n",
       "      <td>150.00</td>\n",
       "      <td>23.86</td>\n",
       "      <td>24.47</td>\n",
       "      <td>23.23</td>\n",
       "      <td>70.44</td>\n",
       "      <td>1.74</td>\n",
       "      <td>Summer</td>\n",
       "      <td>Wet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2014</td>\n",
       "      <td>Abril</td>\n",
       "      <td>85767</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20</td>\n",
       "      <td>22</td>\n",
       "      <td>42</td>\n",
       "      <td>103.92</td>\n",
       "      <td>22.12</td>\n",
       "      <td>22.69</td>\n",
       "      <td>21.59</td>\n",
       "      <td>74.90</td>\n",
       "      <td>1.49</td>\n",
       "      <td>Fall</td>\n",
       "      <td>Dry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2014</td>\n",
       "      <td>Maio</td>\n",
       "      <td>62901</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15</td>\n",
       "      <td>16</td>\n",
       "      <td>31</td>\n",
       "      <td>57.20</td>\n",
       "      <td>19.72</td>\n",
       "      <td>20.28</td>\n",
       "      <td>19.18</td>\n",
       "      <td>70.97</td>\n",
       "      <td>1.44</td>\n",
       "      <td>Fall</td>\n",
       "      <td>Dry</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Ano        Mes  Dengue  Zika  Chik  LeishVis  LeishTeg  LeishT  Precipt  \\\n",
       "0  2014    Janeiro    2901   NaN   NaN        15        31      46   138.90   \n",
       "1  2014  Fevereiro   10196   NaN   NaN        20        32      52   139.67   \n",
       "2  2014      Marco   29647   NaN   NaN        18        29      47   150.00   \n",
       "3  2014      Abril   85767   NaN   NaN        20        22      42   103.92   \n",
       "4  2014       Maio   62901   NaN   NaN        15        16      31    57.20   \n",
       "\n",
       "   AvgTemp  MaxTemp  MinTemp  AvgHumid  AvgWin  Season Season2  \n",
       "0    25.12    25.86    24.41     64.82    1.56  Summer     Wet  \n",
       "1    25.79    26.49    25.10     58.32    1.74  Summer     Wet  \n",
       "2    23.86    24.47    23.23     70.44    1.74  Summer     Wet  \n",
       "3    22.12    22.69    21.59     74.90    1.49    Fall     Dry  \n",
       "4    19.72    20.28    19.18     70.97    1.44    Fall     Dry  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "csv_path = Path(r\"C:\\Users\\idolo\\OneDrive\\Área de Trabalho\\Curso - Modelos Computacionais\\Projeto\") / \"Dataset_Modelos_Computacionais_clean.csv\"\n",
    "\n",
    "df = pd.read_csv(csv_path, encoding=\"utf-8-sig\")\n",
    "\n",
    "\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "083f33c2",
   "metadata": {},
   "source": [
    "##Medidas descritivas (geral)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "90434b4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Ano</th>\n",
       "      <th>Dengue</th>\n",
       "      <th>Zika</th>\n",
       "      <th>Chik</th>\n",
       "      <th>LeishVis</th>\n",
       "      <th>LeishTeg</th>\n",
       "      <th>LeishT</th>\n",
       "      <th>Precipt</th>\n",
       "      <th>AvgTemp</th>\n",
       "      <th>MaxTemp</th>\n",
       "      <th>MinTemp</th>\n",
       "      <th>AvgHumid</th>\n",
       "      <th>AvgWin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>132.000000</td>\n",
       "      <td>132.000000</td>\n",
       "      <td>108.000000</td>\n",
       "      <td>96.000000</td>\n",
       "      <td>132.000000</td>\n",
       "      <td>132.000000</td>\n",
       "      <td>132.000000</td>\n",
       "      <td>132.000000</td>\n",
       "      <td>132.000000</td>\n",
       "      <td>132.000000</td>\n",
       "      <td>132.000000</td>\n",
       "      <td>132.000000</td>\n",
       "      <td>132.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2019.000000</td>\n",
       "      <td>37008.848485</td>\n",
       "      <td>215.425926</td>\n",
       "      <td>927.187500</td>\n",
       "      <td>10.909091</td>\n",
       "      <td>27.848485</td>\n",
       "      <td>38.757576</td>\n",
       "      <td>119.955227</td>\n",
       "      <td>22.179091</td>\n",
       "      <td>22.785227</td>\n",
       "      <td>21.577424</td>\n",
       "      <td>68.240758</td>\n",
       "      <td>1.579091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3.174324</td>\n",
       "      <td>88805.108924</td>\n",
       "      <td>414.422720</td>\n",
       "      <td>1432.025759</td>\n",
       "      <td>4.457526</td>\n",
       "      <td>8.631104</td>\n",
       "      <td>10.511441</td>\n",
       "      <td>80.096108</td>\n",
       "      <td>2.207175</td>\n",
       "      <td>2.269410</td>\n",
       "      <td>2.236895</td>\n",
       "      <td>6.332219</td>\n",
       "      <td>0.223120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2014.000000</td>\n",
       "      <td>310.000000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>96.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>4.970000</td>\n",
       "      <td>16.850000</td>\n",
       "      <td>16.750000</td>\n",
       "      <td>15.650000</td>\n",
       "      <td>50.140000</td>\n",
       "      <td>1.140000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2016.000000</td>\n",
       "      <td>1681.500000</td>\n",
       "      <td>68.750000</td>\n",
       "      <td>265.500000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>31.750000</td>\n",
       "      <td>50.580000</td>\n",
       "      <td>20.165000</td>\n",
       "      <td>20.742500</td>\n",
       "      <td>19.527500</td>\n",
       "      <td>64.165000</td>\n",
       "      <td>1.420000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2019.000000</td>\n",
       "      <td>5134.500000</td>\n",
       "      <td>109.000000</td>\n",
       "      <td>387.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>111.070000</td>\n",
       "      <td>22.955000</td>\n",
       "      <td>23.560000</td>\n",
       "      <td>22.385000</td>\n",
       "      <td>68.915000</td>\n",
       "      <td>1.565000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2022.000000</td>\n",
       "      <td>32834.250000</td>\n",
       "      <td>178.000000</td>\n",
       "      <td>875.250000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>33.250000</td>\n",
       "      <td>46.000000</td>\n",
       "      <td>168.655000</td>\n",
       "      <td>23.912500</td>\n",
       "      <td>24.530000</td>\n",
       "      <td>23.322500</td>\n",
       "      <td>73.152500</td>\n",
       "      <td>1.755000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2024.000000</td>\n",
       "      <td>619390.000000</td>\n",
       "      <td>2874.000000</td>\n",
       "      <td>8154.000000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>54.000000</td>\n",
       "      <td>66.000000</td>\n",
       "      <td>370.270000</td>\n",
       "      <td>25.960000</td>\n",
       "      <td>26.660000</td>\n",
       "      <td>25.290000</td>\n",
       "      <td>79.880000</td>\n",
       "      <td>2.070000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Ano         Dengue         Zika         Chik    LeishVis  \\\n",
       "count   132.000000     132.000000   108.000000    96.000000  132.000000   \n",
       "mean   2019.000000   37008.848485   215.425926   927.187500   10.909091   \n",
       "std       3.174324   88805.108924   414.422720  1432.025759    4.457526   \n",
       "min    2014.000000     310.000000    28.000000    96.000000    2.000000   \n",
       "25%    2016.000000    1681.500000    68.750000   265.500000    7.000000   \n",
       "50%    2019.000000    5134.500000   109.000000   387.000000   11.000000   \n",
       "75%    2022.000000   32834.250000   178.000000   875.250000   14.000000   \n",
       "max    2024.000000  619390.000000  2874.000000  8154.000000   22.000000   \n",
       "\n",
       "         LeishTeg      LeishT     Precipt     AvgTemp     MaxTemp     MinTemp  \\\n",
       "count  132.000000  132.000000  132.000000  132.000000  132.000000  132.000000   \n",
       "mean    27.848485   38.757576  119.955227   22.179091   22.785227   21.577424   \n",
       "std      8.631104   10.511441   80.096108    2.207175    2.269410    2.236895   \n",
       "min      7.000000   12.000000    4.970000   16.850000   16.750000   15.650000   \n",
       "25%     22.000000   31.750000   50.580000   20.165000   20.742500   19.527500   \n",
       "50%     27.000000   38.000000  111.070000   22.955000   23.560000   22.385000   \n",
       "75%     33.250000   46.000000  168.655000   23.912500   24.530000   23.322500   \n",
       "max     54.000000   66.000000  370.270000   25.960000   26.660000   25.290000   \n",
       "\n",
       "         AvgHumid      AvgWin  \n",
       "count  132.000000  132.000000  \n",
       "mean    68.240758    1.579091  \n",
       "std      6.332219    0.223120  \n",
       "min     50.140000    1.140000  \n",
       "25%     64.165000    1.420000  \n",
       "50%     68.915000    1.565000  \n",
       "75%     73.152500    1.755000  \n",
       "max     79.880000    2.070000  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c560abec",
   "metadata": {},
   "source": [
    "## df.info descreve as variáveis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af5c8c5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 132 entries, 0 to 131\n",
      "Data columns (total 16 columns):\n",
      " #   Column    Non-Null Count  Dtype  \n",
      "---  ------    --------------  -----  \n",
      " 0   Ano       132 non-null    int64  \n",
      " 1   Mes       132 non-null    object \n",
      " 2   Dengue    132 non-null    int64  \n",
      " 3   Zika      108 non-null    float64\n",
      " 4   Chik      96 non-null     float64\n",
      " 5   LeishVis  132 non-null    int64  \n",
      " 6   LeishTeg  132 non-null    int64  \n",
      " 7   LeishT    132 non-null    int64  \n",
      " 8   Precipt   132 non-null    float64\n",
      " 9   AvgTemp   132 non-null    float64\n",
      " 10  MaxTemp   132 non-null    float64\n",
      " 11  MinTemp   132 non-null    float64\n",
      " 12  AvgHumid  132 non-null    float64\n",
      " 13  AvgWin    132 non-null    float64\n",
      " 14  Season    132 non-null    object \n",
      " 15  Season2   132 non-null    object \n",
      "dtypes: float64(8), int64(5), object(3)\n",
      "memory usage: 16.6+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9593fcb5",
   "metadata": {},
   "source": [
    "## Pré tratamento dos dados:\n",
    "- Criar uma coluna \"Data\" para armazenar informação de mês e ano (não será uma variável preditora)\n",
    "- Retirar ano;\n",
    "- Retirar mês;\n",
    "- Deixar apenas temperatura média\n",
    "- Excluir variáveis preditoras colineares com VIF>10;\n",
    "- Excluir outliers dos números de casos;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5580a9d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Criando a coluna data\n",
    "# -----------------------------------------------------------\n",
    "# 2. Criar coluna de DATA a partir de Ano + Mes\n",
    "# -----------------------------------------------------------\n",
    "mes_map = {\n",
    "    \"Janeiro\": 1,\n",
    "    \"Fevereiro\": 2,\n",
    "    \"Marco\": 3,\n",
    "    \"Março\": 3,  # caso apareça com acento\n",
    "    \"Abril\": 4,\n",
    "    \"Maio\": 5,\n",
    "    \"Junho\": 6,\n",
    "    \"Julho\": 7,\n",
    "    \"Agosto\": 8,\n",
    "    \"Setembro\": 9,\n",
    "    \"Outubro\": 10,\n",
    "    \"Novembro\": 11,\n",
    "    \"Dezembro\": 12\n",
    "}\n",
    "\n",
    "df[\"MesNum\"] = df[\"Mes\"].map(mes_map)\n",
    "\n",
    "# cria coluna Data (vou usar dia 1 de cada mês)\n",
    "df[\"Data\"] = pd.to_datetime(dict(year=df[\"Ano\"], month=df[\"MesNum\"], day=1))\n",
    "\n",
    "# definir Data como índice temporal e ordenar\n",
    "df = df.set_index(\"Data\").sort_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2a96a0cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Dengue  Zika  Chik  LeishVis  LeishTeg  LeishT  Precipt  AvgTemp  \\\n",
      "Data                                                                           \n",
      "2014-01-01    2901   NaN   NaN        15        31      46   138.90    25.12   \n",
      "2014-02-01   10196   NaN   NaN        20        32      52   139.67    25.79   \n",
      "2014-03-01   29647   NaN   NaN        18        29      47   150.00    23.86   \n",
      "2014-04-01   85767   NaN   NaN        20        22      42   103.92    22.12   \n",
      "2014-05-01   62901   NaN   NaN        15        16      31    57.20    19.72   \n",
      "\n",
      "            AvgHumid  AvgWin  Season Season2  \n",
      "Data                                          \n",
      "2014-01-01     64.82    1.56  Summer     Wet  \n",
      "2014-02-01     58.32    1.74  Summer     Wet  \n",
      "2014-03-01     70.44    1.74  Summer     Wet  \n",
      "2014-04-01     74.90    1.49    Fall     Dry  \n",
      "2014-05-01     70.97    1.44    Fall     Dry  \n"
     ]
    }
   ],
   "source": [
    "## Removendo ano, mês, temperatura máxima e mínima\n",
    "df = df.drop(columns=[\"Ano\", \"Mes\", \"MesNum\", \"MinTemp\", \"MaxTemp\"])\n",
    "\n",
    "\n",
    "# Check the result\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4590e70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Carregando biblioteca para cálculo do VIF\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "4a478132",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tabela de VIF (antes da exclusão):\n",
      "   variavel         VIF\n",
      "0   AvgTemp  111.903118\n",
      "1  AvgHumid   72.214836\n",
      "2    AvgWin   50.060495\n",
      "3   Precipt    4.322273\n",
      "\n",
      "Variáveis com VIF > 10 (candidatas à exclusão):\n",
      "['AvgTemp', 'AvgHumid', 'AvgWin']\n",
      "\n",
      "Colunas finais em df_sem_colinear (após remover VIF > 10):\n",
      "Index(['Dengue', 'Zika', 'Chik', 'LeishVis', 'LeishTeg', 'LeishT', 'Precipt',\n",
      "       'MaxTemp', 'MinTemp', 'Season', 'Season2'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------------------------------\n",
    "# 1. Carregar o dataset\n",
    "# -------------------------------------------------------------------\n",
    "csv_path = Path(r\"C:\\Users\\idolo\\OneDrive\\Área de Trabalho\\Curso - Modelos Computacionais\\Projeto\") / \"Dataset_Modelos_Computacionais_clean.csv\"\n",
    "\n",
    "df = pd.read_csv(csv_path, encoding=\"utf-8-sig\")\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 2. Remover colunas \"Ano\" e \"Mes\" (já decidido no seu projeto)\n",
    "# -------------------------------------------------------------------\n",
    "df = df.drop(columns=[\"Ano\", \"Mes\"])\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 3. Definir quais variáveis vão entrar no cálculo do VIF\n",
    "#    Aqui eu assumo que você quer modelar, por exemplo, DENGUE.\n",
    "#    Então as preditoras são as variáveis climáticas numéricas.\n",
    "# -------------------------------------------------------------------\n",
    "# Se quiser mudar o alvo depois (ex: \"Zika\"), basta trocar aqui.\n",
    "target = \"Dengue\"\n",
    "\n",
    "# Lista de preditoras que vão para o VIF (climáticas)\n",
    "preditoras_vif = [\"Precipt\", \"AvgTemp\", \"AvgHumid\", \"AvgWin\"]\n",
    "\n",
    "# Criar matriz X só com essas colunas (sem valores faltantes)\n",
    "X = df[preditoras_vif].dropna().reset_index(drop=True)\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 4. Calcular o VIF para cada preditora\n",
    "# -------------------------------------------------------------------\n",
    "vif_table = pd.DataFrame()\n",
    "vif_table[\"variavel\"] = preditoras_vif\n",
    "vif_table[\"VIF\"] = [\n",
    "    variance_inflation_factor(X.values, i)\n",
    "    for i in range(X.shape[1])\n",
    "]\n",
    "\n",
    "# Ordenar da maior colinearidade para a menor\n",
    "vif_table = vif_table.sort_values(by=\"VIF\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "print(\"Tabela de VIF (antes da exclusão):\")\n",
    "print(vif_table)\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 5. Identificar variáveis com VIF > 10 e removê-las\n",
    "# -------------------------------------------------------------------\n",
    "limite_vif = 10  # critério da sua anotação\n",
    "\n",
    "vars_alto_vif = vif_table.loc[vif_table[\"VIF\"] > limite_vif, \"variavel\"].tolist()\n",
    "\n",
    "print(\"\\nVariáveis com VIF > 10 (candidatas à exclusão):\")\n",
    "print(vars_alto_vif)\n",
    "\n",
    "# Criar um novo DataFrame SEM essas variáveis colineares\n",
    "df_sem_colinear = df.drop(columns=vars_alto_vif)\n",
    "\n",
    "print(\"\\nColunas finais em df_sem_colinear (após remover VIF > 10):\")\n",
    "print(df_sem_colinear.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8766bd9",
   "metadata": {},
   "source": [
    "## Retirar outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "58c6be65",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9877f92",
   "metadata": {},
   "source": [
    "## Outliers - Método IQR e 99%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c664e3cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Primeiras linhas do df original:\n",
      "    Ano        Mes  Dengue  Zika  Chik  LeishVis  LeishTeg  LeishT  Precipt  \\\n",
      "0  2014    Janeiro    2901   NaN   NaN        15        31      46   138.90   \n",
      "1  2014  Fevereiro   10196   NaN   NaN        20        32      52   139.67   \n",
      "2  2014      Marco   29647   NaN   NaN        18        29      47   150.00   \n",
      "3  2014      Abril   85767   NaN   NaN        20        22      42   103.92   \n",
      "4  2014       Maio   62901   NaN   NaN        15        16      31    57.20   \n",
      "\n",
      "   AvgTemp  MaxTemp  MinTemp  AvgHumid  AvgWin  Season Season2  \n",
      "0    25.12    25.86    24.41     64.82    1.56  Summer     Wet  \n",
      "1    25.79    26.49    25.10     58.32    1.74  Summer     Wet  \n",
      "2    23.86    24.47    23.23     70.44    1.74  Summer     Wet  \n",
      "3    22.12    22.69    21.59     74.90    1.49    Fall     Dry  \n",
      "4    19.72    20.28    19.18     70.97    1.44    Fall     Dry   \n",
      "\n",
      "Resumo das colunas:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 132 entries, 0 to 131\n",
      "Data columns (total 16 columns):\n",
      " #   Column    Non-Null Count  Dtype  \n",
      "---  ------    --------------  -----  \n",
      " 0   Ano       132 non-null    int64  \n",
      " 1   Mes       132 non-null    object \n",
      " 2   Dengue    132 non-null    int64  \n",
      " 3   Zika      108 non-null    float64\n",
      " 4   Chik      96 non-null     float64\n",
      " 5   LeishVis  132 non-null    int64  \n",
      " 6   LeishTeg  132 non-null    int64  \n",
      " 7   LeishT    132 non-null    int64  \n",
      " 8   Precipt   132 non-null    float64\n",
      " 9   AvgTemp   132 non-null    float64\n",
      " 10  MaxTemp   132 non-null    float64\n",
      " 11  MinTemp   132 non-null    float64\n",
      " 12  AvgHumid  132 non-null    float64\n",
      " 13  AvgWin    132 non-null    float64\n",
      " 14  Season    132 non-null    object \n",
      " 15  Season2   132 non-null    object \n",
      "dtypes: float64(8), int64(5), object(3)\n",
      "memory usage: 16.6+ KB\n",
      "None \n",
      "\n",
      "Colunas após pré-tratamento (df):\n",
      "Index(['Dengue', 'Zika', 'Chik', 'LeishVis', 'LeishTeg', 'LeishT', 'Precipt',\n",
      "       'AvgTemp', 'AvgHumid', 'AvgWin', 'Season', 'Season2'],\n",
      "      dtype='object') \n",
      "\n",
      "\n",
      "=== Avaliando outliers (IQR por mês) para: Dengue ===\n",
      "Total de observações para Dengue: 132\n",
      "Número de outliers detectados: 12\n",
      "Outliers de Dengue salvos em: C:\\Users\\idolo\\OneDrive\\Área de Trabalho\\Curso - Modelos Computacionais\\Projeto\\Outliers_RF\\outliers_Dengue_IQR.csv\n",
      "\n",
      "=== Avaliando outliers (IQR por mês) para: Zika ===\n",
      "Total de observações para Zika: 108\n",
      "Número de outliers detectados: 11\n",
      "Outliers de Zika salvos em: C:\\Users\\idolo\\OneDrive\\Área de Trabalho\\Curso - Modelos Computacionais\\Projeto\\Outliers_RF\\outliers_Zika_IQR.csv\n",
      "\n",
      "=== Avaliando outliers (IQR por mês) para: Chik ===\n",
      "Total de observações para Chik: 96\n",
      "Número de outliers detectados: 13\n",
      "Outliers de Chik salvos em: C:\\Users\\idolo\\OneDrive\\Área de Trabalho\\Curso - Modelos Computacionais\\Projeto\\Outliers_RF\\outliers_Chik_IQR.csv\n",
      "\n",
      "=== Avaliando outliers (IQR por mês) para: LeishVis ===\n",
      "Total de observações para LeishVis: 132\n",
      "Número de outliers detectados: 4\n",
      "Outliers de LeishVis salvos em: C:\\Users\\idolo\\OneDrive\\Área de Trabalho\\Curso - Modelos Computacionais\\Projeto\\Outliers_RF\\outliers_LeishVis_IQR.csv\n",
      "\n",
      "=== Avaliando outliers (IQR por mês) para: LeishTeg ===\n",
      "Total de observações para LeishTeg: 132\n",
      "Número de outliers detectados: 12\n",
      "Outliers de LeishTeg salvos em: C:\\Users\\idolo\\OneDrive\\Área de Trabalho\\Curso - Modelos Computacionais\\Projeto\\Outliers_RF\\outliers_LeishTeg_IQR.csv\n",
      "\n",
      "=== Avaliando outliers (IQR por mês) para: LeishT ===\n",
      "Total de observações para LeishT: 132\n",
      "Número de outliers detectados: 7\n",
      "Outliers de LeishT salvos em: C:\\Users\\idolo\\OneDrive\\Área de Trabalho\\Curso - Modelos Computacionais\\Projeto\\Outliers_RF\\outliers_LeishT_IQR.csv\n",
      "\n",
      "Resumo geral de outliers por doença (IQR):\n",
      "     doenca  n_total  n_outliers\n",
      "0    Dengue      132          12\n",
      "1      Zika      108          11\n",
      "2      Chik       96          13\n",
      "3  LeishVis      132           4\n",
      "4  LeishTeg      132          12\n",
      "5    LeishT      132           7\n",
      "\n",
      "Linhas totais em df_out: 132\n",
      "Linhas após remover outliers (IQR por mês): 94\n",
      "\n",
      "Colunas de df_iqr_sem_outliers:\n",
      "Index(['Dengue', 'Zika', 'Chik', 'LeishVis', 'LeishTeg', 'LeishT', 'Precipt',\n",
      "       'AvgTemp', 'AvgHumid', 'AvgWin', 'Season', 'Season2'],\n",
      "      dtype='object') \n",
      "\n",
      "\n",
      "Limiares de 'surto extremo' por doença (p99):\n",
      "     doenca        p99       max\n",
      "0    Dengue  528678.58  619390.0\n",
      "1      Zika    2442.17    2874.0\n",
      "2      Chik    7486.15    8154.0\n",
      "3  LeishVis      21.00      22.0\n",
      "4  LeishTeg      47.69      54.0\n",
      "5    LeishT      62.07      66.0\n",
      "\n",
      "Linhas totais em df_thresh: 132\n",
      "Linhas após remover surtos extremos (p99): 123\n",
      "\n",
      "Arquivo de limiares salvo em: C:\\Users\\idolo\\OneDrive\\Área de Trabalho\\Curso - Modelos Computacionais\\Projeto\\Outliers_RF_p99\\limiares_por_doenca_p99.csv\n",
      "Dataset sem surtos extremos salvo em: C:\\Users\\idolo\\OneDrive\\Área de Trabalho\\Curso - Modelos Computacionais\\Projeto\\Outliers_RF_p99\\Dataset_sem_extremos_p99.csv\n",
      "Outliers de Dengue salvos em: C:\\Users\\idolo\\OneDrive\\Área de Trabalho\\Curso - Modelos Computacionais\\Projeto\\Outliers_RF_p99\\outliers_Dengue_p99.csv\n",
      "Outliers de Zika salvos em: C:\\Users\\idolo\\OneDrive\\Área de Trabalho\\Curso - Modelos Computacionais\\Projeto\\Outliers_RF_p99\\outliers_Zika_p99.csv\n",
      "Outliers de Chik salvos em: C:\\Users\\idolo\\OneDrive\\Área de Trabalho\\Curso - Modelos Computacionais\\Projeto\\Outliers_RF_p99\\outliers_Chik_p99.csv\n",
      "Outliers de LeishVis salvos em: C:\\Users\\idolo\\OneDrive\\Área de Trabalho\\Curso - Modelos Computacionais\\Projeto\\Outliers_RF_p99\\outliers_LeishVis_p99.csv\n",
      "Outliers de LeishTeg salvos em: C:\\Users\\idolo\\OneDrive\\Área de Trabalho\\Curso - Modelos Computacionais\\Projeto\\Outliers_RF_p99\\outliers_LeishTeg_p99.csv\n",
      "Outliers de LeishT salvos em: C:\\Users\\idolo\\OneDrive\\Área de Trabalho\\Curso - Modelos Computacionais\\Projeto\\Outliers_RF_p99\\outliers_LeishT_p99.csv\n",
      "\n",
      "Shape de df_p99_sem_extremos (sem flags *_extremo): (123, 12)\n",
      "Colunas de df_p99_sem_extremos:\n",
      "Index(['Dengue', 'Zika', 'Chik', 'LeishVis', 'LeishTeg', 'LeishT', 'Precipt',\n",
      "       'AvgTemp', 'AvgHumid', 'AvgWin', 'Season', 'Season2'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# =======================================\n",
    "# 0) IMPORTS E CAMINHOS\n",
    "# =======================================\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# Caminho da pasta do projeto (mesmo que você vem usando)\n",
    "base_path = Path(r\"C:\\Users\\idolo\\OneDrive\\Área de Trabalho\\Curso - Modelos Computacionais\\Projeto\")\n",
    "csv_path = base_path / \"Dataset_Modelos_Computacionais_clean.csv\"\n",
    "\n",
    "# =======================================\n",
    "# 1) LEITURA DO CSV E PRÉ-TRATAMENTO BÁSICO\n",
    "#    - cria coluna Data\n",
    "#    - usa Data como índice\n",
    "#    - remove Ano, Mes, MesNum, MinTemp, MaxTemp\n",
    "# =======================================\n",
    "\n",
    "# Leitura\n",
    "df = pd.read_csv(csv_path, encoding=\"utf-8-sig\")\n",
    "\n",
    "# (Opcional) olhar um pouco os dados\n",
    "print(\"Primeiras linhas do df original:\")\n",
    "print(df.head(), \"\\n\")\n",
    "\n",
    "print(\"Resumo das colunas:\")\n",
    "print(df.info(), \"\\n\")\n",
    "\n",
    "# Map de meses em português -> número\n",
    "mes_map = {\n",
    "    \"Janeiro\": 1,\n",
    "    \"Fevereiro\": 2,\n",
    "    \"Marco\": 3,\n",
    "    \"Março\": 3,  # caso apareça com acento\n",
    "    \"Abril\": 4,\n",
    "    \"Maio\": 5,\n",
    "    \"Junho\": 6,\n",
    "    \"Julho\": 7,\n",
    "    \"Agosto\": 8,\n",
    "    \"Setembro\": 9,\n",
    "    \"Outubro\": 10,\n",
    "    \"Novembro\": 11,\n",
    "    \"Dezembro\": 12\n",
    "}\n",
    "\n",
    "# Criar MesNum e coluna Data\n",
    "df[\"MesNum\"] = df[\"Mes\"].map(mes_map)\n",
    "df[\"Data\"] = pd.to_datetime(dict(year=df[\"Ano\"], month=df[\"MesNum\"], day=1))\n",
    "\n",
    "# Usar Data como índice temporal\n",
    "df = df.set_index(\"Data\").sort_index()\n",
    "\n",
    "# Remover colunas que NÃO queremos como preditoras\n",
    "df = df.drop(columns=[\"Ano\", \"Mes\", \"MesNum\", \"MinTemp\", \"MaxTemp\"], errors=\"ignore\")\n",
    "\n",
    "print(\"Colunas após pré-tratamento (df):\")\n",
    "print(df.columns, \"\\n\")\n",
    "\n",
    "# =======================================\n",
    "# 2) OUTLIERS POR IQR (POR MÊS)\n",
    "#    -> df_iqr_sem_outliers\n",
    "#    + CSVs em pasta Outliers_RF\n",
    "# =======================================\n",
    "\n",
    "# Copia de trabalho\n",
    "df_out = df.copy()\n",
    "df_out.index = pd.to_datetime(df_out.index)\n",
    "\n",
    "# Coluna explícita de Data para exportar\n",
    "df_out[\"Data\"] = df_out.index\n",
    "\n",
    "# Mês do ano (1–12) e ano (pra análise IQR e CSV)\n",
    "df_out[\"Mes_do_ano\"] = df_out[\"Data\"].dt.month\n",
    "df_out[\"Ano\"] = df_out[\"Data\"].dt.year\n",
    "\n",
    "# Lista de doenças\n",
    "disease_cols = [\"Dengue\", \"Zika\", \"Chik\", \"LeishVis\", \"LeishTeg\", \"LeishT\"]\n",
    "\n",
    "# Pasta para salvar os CSVs de outliers IQR\n",
    "out_dir_iqr = base_path / \"Outliers_RF\"\n",
    "out_dir_iqr.mkdir(exist_ok=True)\n",
    "\n",
    "resumo_outliers = []\n",
    "\n",
    "for col in disease_cols:\n",
    "    if col not in df_out.columns:\n",
    "        print(f\"[AVISO] {col} não está no dataframe. Pulando.\")\n",
    "        continue\n",
    "\n",
    "    print(f\"\\n=== Avaliando outliers (IQR por mês) para: {col} ===\")\n",
    "\n",
    "    sub = df_out[[\"Data\", \"Ano\", \"Mes_do_ano\", col]].copy()\n",
    "    sub = sub.dropna(subset=[col])\n",
    "\n",
    "    if sub.empty:\n",
    "        print(f\"[AVISO] {col} só tem NaN. Pulando.\")\n",
    "        continue\n",
    "\n",
    "    # Q1, Q3 e IQR por mês do ano\n",
    "    grupo = sub.groupby(\"Mes_do_ano\")[col]\n",
    "    q1 = grupo.quantile(0.25)\n",
    "    q3 = grupo.quantile(0.75)\n",
    "\n",
    "    sub = sub.join(q1.rename(\"Q1\"), on=\"Mes_do_ano\")\n",
    "    sub = sub.join(q3.rename(\"Q3\"), on=\"Mes_do_ano\")\n",
    "    sub[\"IQR\"] = sub[\"Q3\"] - sub[\"Q1\"]\n",
    "\n",
    "    sub[\"Limite_inferior\"] = sub[\"Q1\"] - 1.5 * sub[\"IQR\"]\n",
    "    sub[\"Limite_superior\"] = sub[\"Q3\"] + 1.5 * sub[\"IQR\"]\n",
    "\n",
    "    sub[\"is_outlier\"] = (sub[col] < sub[\"Limite_inferior\"]) | (sub[col] > sub[\"Limite_superior\"])\n",
    "\n",
    "    outliers = sub[sub[\"is_outlier\"]].copy()\n",
    "\n",
    "    print(f\"Total de observações para {col}: {len(sub)}\")\n",
    "    print(f\"Número de outliers detectados: {len(outliers)}\")\n",
    "\n",
    "    resumo_outliers.append({\n",
    "        \"doenca\": col,\n",
    "        \"n_total\": len(sub),\n",
    "        \"n_outliers\": len(outliers)\n",
    "    })\n",
    "\n",
    "    # salvar CSV só de outliers dessa doença\n",
    "    if len(outliers) > 0:\n",
    "        cols_salvar = [\n",
    "            \"Data\", \"Ano\", \"Mes_do_ano\", col,\n",
    "            \"Q1\", \"Q3\", \"Limite_inferior\", \"Limite_superior\", \"is_outlier\"\n",
    "        ]\n",
    "        out_csv_path = out_dir_iqr / f\"outliers_{col}_IQR.csv\"\n",
    "        outliers[cols_salvar].to_csv(out_csv_path, index=False, encoding=\"utf-8-sig\")\n",
    "        print(f\"Outliers de {col} salvos em: {out_csv_path}\")\n",
    "    else:\n",
    "        print(f\"Nenhum outlier detectado para {col} com esse critério.\")\n",
    "\n",
    "    # Escrever flag de outlier de volta no df_out (alinhando por Data)\n",
    "    flag_series = sub.set_index(\"Data\")[\"is_outlier\"]\n",
    "    df_out[f\"{col}_outlier_IQR\"] = df_out.index.isin(flag_series[flag_series].index)\n",
    "\n",
    "# Resumo geral IQR\n",
    "resumo_iqr_df = pd.DataFrame(resumo_outliers)\n",
    "print(\"\\nResumo geral de outliers por doença (IQR):\")\n",
    "print(resumo_iqr_df)\n",
    "\n",
    "# Construir df_iqr_sem_outliers (remover qualquer linha com outlier em qualquer doença)\n",
    "iqr_flag_cols = [c for c in df_out.columns if c.endswith(\"_outlier_IQR\")]\n",
    "\n",
    "if len(iqr_flag_cols) > 0:\n",
    "    mask_any_iqr = df_out[iqr_flag_cols].any(axis=1)\n",
    "    df_iqr_sem_outliers = df_out[~mask_any_iqr].copy()\n",
    "else:\n",
    "    df_iqr_sem_outliers = df_out.copy()\n",
    "\n",
    "print(\"\\nLinhas totais em df_out:\", len(df_out))\n",
    "print(\"Linhas após remover outliers (IQR por mês):\", len(df_iqr_sem_outliers))\n",
    "\n",
    "# Limpar colunas auxiliares do df_iqr_sem_outliers para modelagem\n",
    "df_iqr_sem_outliers = df_iqr_sem_outliers.drop(\n",
    "    columns=iqr_flag_cols + [\"Mes_do_ano\", \"Ano\", \"Data\"],\n",
    "    errors=\"ignore\"\n",
    ")\n",
    "\n",
    "print(\"\\nColunas de df_iqr_sem_outliers:\")\n",
    "print(df_iqr_sem_outliers.columns, \"\\n\")\n",
    "\n",
    "# =======================================\n",
    "# 3) OUTLIERS \"EXTREMOS\" POR PERCENTIL GLOBAL (p99)\n",
    "#    -> df_p99_sem_extremos\n",
    "#    + pasta Outliers_RF_p99\n",
    "# =======================================\n",
    "\n",
    "df_thresh = df.copy()\n",
    "df_thresh.index = pd.to_datetime(df_thresh.index)\n",
    "\n",
    "percentile_cutoff = 0.99  # 99º percentil\n",
    "thresholds = []\n",
    "\n",
    "for col in disease_cols:\n",
    "    if col not in df_thresh.columns:\n",
    "        print(f\"[AVISO] {col} não está no dataframe. Pulando.\")\n",
    "        continue\n",
    "\n",
    "    s = df_thresh[col].dropna()\n",
    "\n",
    "    if s.empty:\n",
    "        print(f\"[AVISO] {col} só tem NaN. Pulando.\")\n",
    "        continue\n",
    "\n",
    "    # X: valor acima do qual vamos considerar \"surto extremo\"\n",
    "    X = s.quantile(percentile_cutoff)\n",
    "\n",
    "    thresholds.append({\n",
    "        \"doenca\": col,\n",
    "        f\"p{int(percentile_cutoff*100)}\": X,\n",
    "        \"max\": s.max()\n",
    "    })\n",
    "\n",
    "    # cria uma flag de \"surto extremo\" para essa doença\n",
    "    df_thresh[f\"{col}_extremo\"] = df_thresh[col] > X\n",
    "\n",
    "# tabela com cutoffs por doença\n",
    "thresholds_df = pd.DataFrame(thresholds)\n",
    "print(\"\\nLimiares de 'surto extremo' por doença (p99):\")\n",
    "print(thresholds_df)\n",
    "\n",
    "# construir df_sem_extremos removendo qualquer linha com *_extremo = True\n",
    "extreme_flag_cols = [c for c in df_thresh.columns if c.endswith(\"_extremo\")]\n",
    "\n",
    "if len(extreme_flag_cols) > 0:\n",
    "    mask_any_extreme = df_thresh[extreme_flag_cols].any(axis=1)\n",
    "    df_sem_extremos = df_thresh[~mask_any_extreme].copy()\n",
    "else:\n",
    "    df_sem_extremos = df_thresh.copy()\n",
    "\n",
    "print(\"\\nLinhas totais em df_thresh:\", len(df_thresh))\n",
    "print(\"Linhas após remover surtos extremos (p99):\", len(df_sem_extremos))\n",
    "\n",
    "# CRIAR PASTA E SALVAR CSVs DO P99\n",
    "out_dir_percentil = base_path / f\"Outliers_RF_p{int(percentile_cutoff*100)}\"\n",
    "out_dir_percentil.mkdir(exist_ok=True)\n",
    "\n",
    "# 1) salvar limiares por doença\n",
    "thresholds_csv_path = out_dir_percentil / f\"limiares_por_doenca_p{int(percentile_cutoff*100)}.csv\"\n",
    "thresholds_df.to_csv(thresholds_csv_path, index=False, encoding=\"utf-8-sig\")\n",
    "print(f\"\\nArquivo de limiares salvo em: {thresholds_csv_path}\")\n",
    "\n",
    "# 2) salvar dataset sem surtos extremos (incluindo flags *_extremo)\n",
    "dataset_sem_extremos_path = out_dir_percentil / f\"Dataset_sem_extremos_p{int(percentile_cutoff*100)}.csv\"\n",
    "df_sem_extremos.to_csv(dataset_sem_extremos_path, encoding=\"utf-8-sig\", index=True)\n",
    "print(f\"Dataset sem surtos extremos salvo em: {dataset_sem_extremos_path}\")\n",
    "\n",
    "# 3) salvar CSV de outliers por doença\n",
    "df_export = df_thresh.copy()\n",
    "df_export[\"Data\"] = df_export.index\n",
    "\n",
    "for col in disease_cols:\n",
    "    flag_col = f\"{col}_extremo\"\n",
    "    if flag_col not in df_export.columns:\n",
    "        continue\n",
    "\n",
    "    outliers_col = df_export[df_export[flag_col]].copy()\n",
    "    if outliers_col.empty:\n",
    "        print(f\"Nenhum surto extremo para {col} com p{int(percentile_cutoff*100)}.\")\n",
    "        continue\n",
    "\n",
    "    cols_salvar = [\"Data\", col, flag_col]\n",
    "    out_csv_path = out_dir_percentil / f\"outliers_{col}_p{int(percentile_cutoff*100)}.csv\"\n",
    "    outliers_col[cols_salvar].to_csv(out_csv_path, index=False, encoding=\"utf-8-sig\")\n",
    "    print(f\"Outliers de {col} salvos em: {out_csv_path}\")\n",
    "\n",
    "# 4) DataFrame final para usar nos modelos (sem flags *_extremo)\n",
    "df_p99_sem_extremos = df_sem_extremos.drop(columns=extreme_flag_cols + [\"Data\"], errors=\"ignore\")\n",
    "\n",
    "print(\"\\nShape de df_p99_sem_extremos (sem flags *_extremo):\", df_p99_sem_extremos.shape)\n",
    "print(\"Colunas de df_p99_sem_extremos:\")\n",
    "print(df_p99_sem_extremos.columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8afb1125",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ae8eeb35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# IMPORTS E CAMINHO BASE\n",
    "# =====================================================\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f0bd43",
   "metadata": {},
   "source": [
    "## Random Forest - Todos os modelos - CORRETO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8c17d85d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> Processando Dataset: base (No Season) <<<\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\idolo\\anaconda3\\Lib\\site-packages\\seaborn\\_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n",
      "  with pd.option_context('mode.use_inf_as_na', True):\n",
      "c:\\Users\\idolo\\anaconda3\\Lib\\site-packages\\seaborn\\_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n",
      "  with pd.option_context('mode.use_inf_as_na', True):\n",
      "c:\\Users\\idolo\\anaconda3\\Lib\\site-packages\\seaborn\\_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n",
      "  with pd.option_context('mode.use_inf_as_na', True):\n",
      "c:\\Users\\idolo\\anaconda3\\Lib\\site-packages\\seaborn\\_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n",
      "  with pd.option_context('mode.use_inf_as_na', True):\n",
      "c:\\Users\\idolo\\anaconda3\\Lib\\site-packages\\seaborn\\_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n",
      "  with pd.option_context('mode.use_inf_as_na', True):\n",
      "c:\\Users\\idolo\\anaconda3\\Lib\\site-packages\\seaborn\\_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n",
      "  with pd.option_context('mode.use_inf_as_na', True):\n",
      "c:\\Users\\idolo\\anaconda3\\Lib\\site-packages\\seaborn\\_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n",
      "  with pd.option_context('mode.use_inf_as_na', True):\n",
      "c:\\Users\\idolo\\anaconda3\\Lib\\site-packages\\seaborn\\_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n",
      "  with pd.option_context('mode.use_inf_as_na', True):\n",
      "c:\\Users\\idolo\\anaconda3\\Lib\\site-packages\\seaborn\\_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n",
      "  with pd.option_context('mode.use_inf_as_na', True):\n",
      "c:\\Users\\idolo\\anaconda3\\Lib\\site-packages\\seaborn\\_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n",
      "  with pd.option_context('mode.use_inf_as_na', True):\n",
      "c:\\Users\\idolo\\anaconda3\\Lib\\site-packages\\seaborn\\_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n",
      "  with pd.option_context('mode.use_inf_as_na', True):\n",
      "c:\\Users\\idolo\\anaconda3\\Lib\\site-packages\\seaborn\\_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n",
      "  with pd.option_context('mode.use_inf_as_na', True):\n",
      "c:\\Users\\idolo\\anaconda3\\Lib\\site-packages\\seaborn\\_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n",
      "  with pd.option_context('mode.use_inf_as_na', True):\n",
      "c:\\Users\\idolo\\anaconda3\\Lib\\site-packages\\seaborn\\_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n",
      "  with pd.option_context('mode.use_inf_as_na', True):\n",
      "c:\\Users\\idolo\\anaconda3\\Lib\\site-packages\\seaborn\\_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n",
      "  with pd.option_context('mode.use_inf_as_na', True):\n",
      "c:\\Users\\idolo\\anaconda3\\Lib\\site-packages\\seaborn\\_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n",
      "  with pd.option_context('mode.use_inf_as_na', True):\n",
      "c:\\Users\\idolo\\anaconda3\\Lib\\site-packages\\seaborn\\_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n",
      "  with pd.option_context('mode.use_inf_as_na', True):\n",
      "c:\\Users\\idolo\\anaconda3\\Lib\\site-packages\\seaborn\\_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n",
      "  with pd.option_context('mode.use_inf_as_na', True):\n",
      "c:\\Users\\idolo\\anaconda3\\Lib\\site-packages\\seaborn\\_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n",
      "  with pd.option_context('mode.use_inf_as_na', True):\n",
      "c:\\Users\\idolo\\anaconda3\\Lib\\site-packages\\seaborn\\_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n",
      "  with pd.option_context('mode.use_inf_as_na', True):\n",
      "c:\\Users\\idolo\\anaconda3\\Lib\\site-packages\\seaborn\\_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n",
      "  with pd.option_context('mode.use_inf_as_na', True):\n",
      "c:\\Users\\idolo\\anaconda3\\Lib\\site-packages\\seaborn\\_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n",
      "  with pd.option_context('mode.use_inf_as_na', True):\n",
      "c:\\Users\\idolo\\anaconda3\\Lib\\site-packages\\seaborn\\_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n",
      "  with pd.option_context('mode.use_inf_as_na', True):\n",
      "c:\\Users\\idolo\\anaconda3\\Lib\\site-packages\\seaborn\\_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n",
      "  with pd.option_context('mode.use_inf_as_na', True):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> Processando Dataset: IQR (No Season) <<<\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\idolo\\anaconda3\\Lib\\site-packages\\seaborn\\_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n",
      "  with pd.option_context('mode.use_inf_as_na', True):\n",
      "c:\\Users\\idolo\\anaconda3\\Lib\\site-packages\\seaborn\\_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n",
      "  with pd.option_context('mode.use_inf_as_na', True):\n",
      "c:\\Users\\idolo\\anaconda3\\Lib\\site-packages\\seaborn\\_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n",
      "  with pd.option_context('mode.use_inf_as_na', True):\n",
      "c:\\Users\\idolo\\anaconda3\\Lib\\site-packages\\seaborn\\_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n",
      "  with pd.option_context('mode.use_inf_as_na', True):\n",
      "c:\\Users\\idolo\\anaconda3\\Lib\\site-packages\\seaborn\\_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n",
      "  with pd.option_context('mode.use_inf_as_na', True):\n",
      "c:\\Users\\idolo\\anaconda3\\Lib\\site-packages\\seaborn\\_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n",
      "  with pd.option_context('mode.use_inf_as_na', True):\n",
      "c:\\Users\\idolo\\anaconda3\\Lib\\site-packages\\seaborn\\_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n",
      "  with pd.option_context('mode.use_inf_as_na', True):\n",
      "c:\\Users\\idolo\\anaconda3\\Lib\\site-packages\\seaborn\\_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n",
      "  with pd.option_context('mode.use_inf_as_na', True):\n",
      "c:\\Users\\idolo\\anaconda3\\Lib\\site-packages\\seaborn\\_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n",
      "  with pd.option_context('mode.use_inf_as_na', True):\n",
      "c:\\Users\\idolo\\anaconda3\\Lib\\site-packages\\seaborn\\_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n",
      "  with pd.option_context('mode.use_inf_as_na', True):\n",
      "c:\\Users\\idolo\\anaconda3\\Lib\\site-packages\\seaborn\\_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n",
      "  with pd.option_context('mode.use_inf_as_na', True):\n",
      "c:\\Users\\idolo\\anaconda3\\Lib\\site-packages\\seaborn\\_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n",
      "  with pd.option_context('mode.use_inf_as_na', True):\n",
      "c:\\Users\\idolo\\anaconda3\\Lib\\site-packages\\seaborn\\_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n",
      "  with pd.option_context('mode.use_inf_as_na', True):\n",
      "c:\\Users\\idolo\\anaconda3\\Lib\\site-packages\\seaborn\\_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n",
      "  with pd.option_context('mode.use_inf_as_na', True):\n",
      "c:\\Users\\idolo\\anaconda3\\Lib\\site-packages\\seaborn\\_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n",
      "  with pd.option_context('mode.use_inf_as_na', True):\n",
      "c:\\Users\\idolo\\anaconda3\\Lib\\site-packages\\seaborn\\_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n",
      "  with pd.option_context('mode.use_inf_as_na', True):\n",
      "c:\\Users\\idolo\\anaconda3\\Lib\\site-packages\\seaborn\\_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n",
      "  with pd.option_context('mode.use_inf_as_na', True):\n",
      "c:\\Users\\idolo\\anaconda3\\Lib\\site-packages\\seaborn\\_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n",
      "  with pd.option_context('mode.use_inf_as_na', True):\n",
      "c:\\Users\\idolo\\anaconda3\\Lib\\site-packages\\seaborn\\_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n",
      "  with pd.option_context('mode.use_inf_as_na', True):\n",
      "c:\\Users\\idolo\\anaconda3\\Lib\\site-packages\\seaborn\\_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n",
      "  with pd.option_context('mode.use_inf_as_na', True):\n",
      "c:\\Users\\idolo\\anaconda3\\Lib\\site-packages\\seaborn\\_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n",
      "  with pd.option_context('mode.use_inf_as_na', True):\n",
      "c:\\Users\\idolo\\anaconda3\\Lib\\site-packages\\seaborn\\_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n",
      "  with pd.option_context('mode.use_inf_as_na', True):\n",
      "c:\\Users\\idolo\\anaconda3\\Lib\\site-packages\\seaborn\\_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n",
      "  with pd.option_context('mode.use_inf_as_na', True):\n",
      "c:\\Users\\idolo\\anaconda3\\Lib\\site-packages\\seaborn\\_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n",
      "  with pd.option_context('mode.use_inf_as_na', True):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> Processando Dataset: p99 (No Season) <<<\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\idolo\\anaconda3\\Lib\\site-packages\\seaborn\\_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n",
      "  with pd.option_context('mode.use_inf_as_na', True):\n",
      "c:\\Users\\idolo\\anaconda3\\Lib\\site-packages\\seaborn\\_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n",
      "  with pd.option_context('mode.use_inf_as_na', True):\n",
      "c:\\Users\\idolo\\anaconda3\\Lib\\site-packages\\seaborn\\_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n",
      "  with pd.option_context('mode.use_inf_as_na', True):\n",
      "c:\\Users\\idolo\\anaconda3\\Lib\\site-packages\\seaborn\\_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n",
      "  with pd.option_context('mode.use_inf_as_na', True):\n",
      "c:\\Users\\idolo\\anaconda3\\Lib\\site-packages\\seaborn\\_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n",
      "  with pd.option_context('mode.use_inf_as_na', True):\n",
      "c:\\Users\\idolo\\anaconda3\\Lib\\site-packages\\seaborn\\_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n",
      "  with pd.option_context('mode.use_inf_as_na', True):\n",
      "c:\\Users\\idolo\\anaconda3\\Lib\\site-packages\\seaborn\\_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n",
      "  with pd.option_context('mode.use_inf_as_na', True):\n",
      "c:\\Users\\idolo\\anaconda3\\Lib\\site-packages\\seaborn\\_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n",
      "  with pd.option_context('mode.use_inf_as_na', True):\n",
      "c:\\Users\\idolo\\anaconda3\\Lib\\site-packages\\seaborn\\_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n",
      "  with pd.option_context('mode.use_inf_as_na', True):\n",
      "c:\\Users\\idolo\\anaconda3\\Lib\\site-packages\\seaborn\\_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n",
      "  with pd.option_context('mode.use_inf_as_na', True):\n",
      "c:\\Users\\idolo\\anaconda3\\Lib\\site-packages\\seaborn\\_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n",
      "  with pd.option_context('mode.use_inf_as_na', True):\n",
      "c:\\Users\\idolo\\anaconda3\\Lib\\site-packages\\seaborn\\_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n",
      "  with pd.option_context('mode.use_inf_as_na', True):\n",
      "c:\\Users\\idolo\\anaconda3\\Lib\\site-packages\\seaborn\\_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n",
      "  with pd.option_context('mode.use_inf_as_na', True):\n",
      "c:\\Users\\idolo\\anaconda3\\Lib\\site-packages\\seaborn\\_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n",
      "  with pd.option_context('mode.use_inf_as_na', True):\n",
      "c:\\Users\\idolo\\anaconda3\\Lib\\site-packages\\seaborn\\_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n",
      "  with pd.option_context('mode.use_inf_as_na', True):\n",
      "c:\\Users\\idolo\\anaconda3\\Lib\\site-packages\\seaborn\\_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n",
      "  with pd.option_context('mode.use_inf_as_na', True):\n",
      "c:\\Users\\idolo\\anaconda3\\Lib\\site-packages\\seaborn\\_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n",
      "  with pd.option_context('mode.use_inf_as_na', True):\n",
      "c:\\Users\\idolo\\anaconda3\\Lib\\site-packages\\seaborn\\_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n",
      "  with pd.option_context('mode.use_inf_as_na', True):\n",
      "c:\\Users\\idolo\\anaconda3\\Lib\\site-packages\\seaborn\\_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n",
      "  with pd.option_context('mode.use_inf_as_na', True):\n",
      "c:\\Users\\idolo\\anaconda3\\Lib\\site-packages\\seaborn\\_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n",
      "  with pd.option_context('mode.use_inf_as_na', True):\n",
      "c:\\Users\\idolo\\anaconda3\\Lib\\site-packages\\seaborn\\_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n",
      "  with pd.option_context('mode.use_inf_as_na', True):\n",
      "c:\\Users\\idolo\\anaconda3\\Lib\\site-packages\\seaborn\\_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n",
      "  with pd.option_context('mode.use_inf_as_na', True):\n",
      "c:\\Users\\idolo\\anaconda3\\Lib\\site-packages\\seaborn\\_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n",
      "  with pd.option_context('mode.use_inf_as_na', True):\n",
      "c:\\Users\\idolo\\anaconda3\\Lib\\site-packages\\seaborn\\_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n",
      "  with pd.option_context('mode.use_inf_as_na', True):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== RESUMO GERAL DE TODOS OS MODELOS (NO SEASON) ===\n",
      "                  modelo dataset    doenca           MAE           MSE  \\\n",
      "0  M1_aleatorio_NoSeason    base    Dengue  22676.013771  1.338383e+09   \n",
      "1  M1_aleatorio_NoSeason    base      Zika    142.099433  7.661501e+04   \n",
      "2  M1_aleatorio_NoSeason    base      Chik    651.053637  7.568262e+05   \n",
      "3  M1_aleatorio_NoSeason    base  LeishVis      3.783511  2.095052e+01   \n",
      "4  M1_aleatorio_NoSeason    base  LeishTeg      8.275609  1.067565e+02   \n",
      "\n",
      "         R2  \n",
      "0 -0.727301  \n",
      "1 -0.629034  \n",
      "2 -0.596153  \n",
      "3  0.042267  \n",
      "4 -0.326418  \n",
      "\n",
      "Arquivo Excel final salvo em: C:\\Users\\idolo\\OneDrive\\Área de Trabalho\\Curso - Modelos Computacionais\\Projeto\\Resultados_Gerais_NoSeason\\Tabela_Final_RF_Modelos_NoSeason.xlsx\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# =====================================================\n",
    "# CONFIGURAÇÕES INICIAIS\n",
    "# =====================================================\n",
    "base_path = Path(r\"C:\\Users\\idolo\\OneDrive\\Área de Trabalho\\Curso - Modelos Computacionais\\Projeto\")\n",
    "\n",
    "# Nome da pasta mãe que vai conter tudo dessa rodada\n",
    "pasta_mae_nome = \"Resultados_Gerais_NoSeason\"\n",
    "\n",
    "possible_disease_cols = [\"Dengue\", \"Zika\", \"Chik\", \"LeishVis\", \"LeishTeg\", \"LeishT\"]\n",
    "climate_cols = [\"Precipt\", \"AvgTemp\", \"MaxTemp\", \"MinTemp\", \"AvgHumid\", \"AvgWin\"]\n",
    "\n",
    "# =====================================================\n",
    "# FUNÇÕES AUXILIARES\n",
    "# =====================================================\n",
    "\n",
    "def calcular_importancia_agregada(model, numeric_features, categorical_features):\n",
    "    rf = model.named_steps[\"rf\"]\n",
    "\n",
    "    cat_names = []\n",
    "    try:\n",
    "        ohe = model.named_steps[\"preprocess\"].named_transformers_[\"cat\"]\n",
    "        if len(categorical_features) > 0:\n",
    "            cat_names = list(ohe.get_feature_names_out(categorical_features))\n",
    "    except Exception:\n",
    "        cat_names = []\n",
    "\n",
    "    num_names = list(numeric_features)\n",
    "    all_feature_names = num_names + cat_names\n",
    "\n",
    "    importances = pd.Series(rf.feature_importances_, index=all_feature_names)\n",
    "\n",
    "    agg = {}\n",
    "    for feat, imp in importances.items():\n",
    "        if feat in cat_names:\n",
    "            base = feat.split(\"_\", 1)[0]\n",
    "        else:\n",
    "            base = feat\n",
    "        agg[base] = agg.get(base, 0.0) + imp\n",
    "\n",
    "    agg_series = pd.Series(agg).sort_values(ascending=False)\n",
    "    return agg_series\n",
    "\n",
    "\n",
    "def fazer_grafico_3plots(y_test, y_pred, agg_series, mae, mse, r2, target_col, descricao_dataset, titulo_extra, file_path):\n",
    "    residuals = y_test - y_pred\n",
    "    top_10 = agg_series.head(10).sort_values(ascending=True)\n",
    "\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(25, 8))\n",
    "\n",
    "    # Plot 1: Real vs Previsto\n",
    "    ax1 = axes[0]\n",
    "    sns.scatterplot(x=y_test, y=y_pred, ax=ax1)\n",
    "    min_val = min(y_test.min(), y_pred.min())\n",
    "    max_val = max(y_test.max(), y_pred.max())\n",
    "    ax1.plot([min_val, max_val], [min_val, max_val], linestyle=\"--\", color=\"red\")\n",
    "    ax1.set_xlabel(\"Casos reais\")\n",
    "    ax1.set_ylabel(\"Casos previstos\")\n",
    "    ax1.set_title(f\"Real vs Previsto — {target_col} {titulo_extra}\")\n",
    "    ax1.text(0.05, 0.95, f\"MAE = {mae:.2f}\\nMSE = {mse:.2f}\\nR² = {r2:.3f}\", transform=ax1.transAxes, va=\"top\")\n",
    "\n",
    "    # Plot 2: Resíduos\n",
    "    ax2 = axes[1]\n",
    "    sns.histplot(residuals, bins=20, ax=ax2, kde=True)\n",
    "    ax2.set_xlabel(\"Resíduos (Real − Previsto)\")\n",
    "    ax2.set_ylabel(\"Frequência\")\n",
    "    ax2.set_title(\"Distribuição dos Resíduos\")\n",
    "\n",
    "    # Plot 3: Importância\n",
    "    ax3 = axes[2]\n",
    "    ax3.barh(top_10.index, top_10.values)\n",
    "    ax3.set_xlabel(\"Importância (agregada)\")\n",
    "    ax3.set_ylabel(\"Variáveis\")\n",
    "    ax3.set_title(\"Top 10 Variáveis Mais Importantes\")\n",
    "\n",
    "    fig.suptitle(f\"Random Forest (NoSeason) — {target_col} [{descricao_dataset}] {titulo_extra}\", y=1.02, fontsize=14)\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(file_path, dpi=300, bbox_inches=\"tight\")\n",
    "    plt.close(fig)\n",
    "\n",
    "# =====================================================\n",
    "# MODELO 1 – RF SPLIT ALEATÓRIO\n",
    "# =====================================================\n",
    "\n",
    "def rodar_rf_basico_para_df(df_input, output_folder_path, descricao_dataset):\n",
    "    df_model = df_input.copy()\n",
    "    df_model.index = pd.to_datetime(df_model.index)\n",
    "\n",
    "    # >>> AQUI REMOVEMOS SEASON E SEASON2 <<<\n",
    "    cols_to_drop = []\n",
    "    lista_remocao = [\"Data\", \"Ano\", \"Mes\", \"MesNum\", \"Mes_do_ano\", \"Season\", \"Season2\"]\n",
    "    \n",
    "    for c in lista_remocao:\n",
    "        if c in df_model.columns:\n",
    "            cols_to_drop.append(c)\n",
    "\n",
    "    extra_drop = [c for c in df_model.columns if c.endswith(\"_outlier_IQR\") or c.endswith(\"_extremo\")]\n",
    "    cols_to_drop.extend(extra_drop)\n",
    "\n",
    "    if cols_to_drop:\n",
    "        df_model = df_model.drop(columns=cols_to_drop, errors=\"ignore\")\n",
    "\n",
    "    # Caminho completo incluindo a pasta mãe\n",
    "    output_dir = base_path / output_folder_path\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    resultados = []\n",
    "\n",
    "    for target_col in possible_disease_cols:\n",
    "        if target_col not in df_model.columns:\n",
    "            continue\n",
    "\n",
    "        first_idx = df_model[target_col].first_valid_index()\n",
    "        if first_idx is None:\n",
    "            continue\n",
    "\n",
    "        df_target = df_model.loc[first_idx:].copy()\n",
    "        y = df_target[target_col]\n",
    "        X = df_target.drop(columns=[c for c in possible_disease_cols if c in df_target.columns], errors=\"ignore\")\n",
    "\n",
    "        mask_valid = y.notna() & ~X.isna().any(axis=1)\n",
    "        X = X[mask_valid]\n",
    "        y = y[mask_valid]\n",
    "\n",
    "        if len(X) < 10:\n",
    "            continue\n",
    "\n",
    "        numeric_features = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "        categorical_features = X.select_dtypes(exclude=[np.number]).columns.tolist()\n",
    "\n",
    "        preprocess = ColumnTransformer([\n",
    "            (\"num\", \"passthrough\", numeric_features),\n",
    "            (\"cat\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False), categorical_features),\n",
    "        ], remainder=\"drop\")\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "\n",
    "        model = Pipeline(steps=[\n",
    "            (\"preprocess\", preprocess),\n",
    "            (\"rf\", RandomForestRegressor(n_estimators=999, random_state=42, n_jobs=-1))\n",
    "        ])\n",
    "\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "\n",
    "        mae = mean_absolute_error(y_test, y_pred)\n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        r2  = r2_score(y_test, y_pred)\n",
    "\n",
    "        agg_series = calcular_importancia_agregada(model, numeric_features, categorical_features)\n",
    "\n",
    "        resultados.append({\n",
    "            \"modelo\": \"M1_aleatorio_NoSeason\",\n",
    "            \"dataset\": descricao_dataset,\n",
    "            \"doenca\": target_col,\n",
    "            \"MAE\": mae, \"MSE\": mse, \"R2\": r2\n",
    "        })\n",
    "\n",
    "        fazer_grafico_3plots(y_test, y_pred, agg_series, mae, mse, r2, target_col, descricao_dataset, \"(split aleatório)\", output_dir / f\"RF_{target_col}_M1.png\")\n",
    "\n",
    "    return pd.DataFrame(resultados)\n",
    "\n",
    "# =====================================================\n",
    "# MODELO 2 – RF COM SPLIT TEMPORAL\n",
    "# =====================================================\n",
    "\n",
    "def rodar_rf_temporal_para_df(df_input, output_folder_path, descricao_dataset):\n",
    "    df_model = df_input.copy()\n",
    "    df_model.index = pd.to_datetime(df_model.index)\n",
    "    df_model = df_model.sort_index()\n",
    "\n",
    "    # >>> AQUI REMOVEMOS SEASON E SEASON2 <<<\n",
    "    cols_to_drop = []\n",
    "    lista_remocao = [\"Data\", \"Ano\", \"Mes\", \"MesNum\", \"Mes_do_ano\", \"Season\", \"Season2\"]\n",
    "    \n",
    "    for c in lista_remocao:\n",
    "        if c in df_model.columns:\n",
    "            cols_to_drop.append(c)\n",
    "\n",
    "    extra_drop = [c for c in df_model.columns if c.endswith(\"_outlier_IQR\") or c.endswith(\"_extremo\")]\n",
    "    cols_to_drop.extend(extra_drop)\n",
    "\n",
    "    if cols_to_drop:\n",
    "        df_model = df_model.drop(columns=cols_to_drop, errors=\"ignore\")\n",
    "\n",
    "    output_dir = base_path / output_folder_path\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    resultados = []\n",
    "\n",
    "    for target_col in possible_disease_cols:\n",
    "        if target_col not in df_model.columns: continue\n",
    "        \n",
    "        first_idx = df_model[target_col].first_valid_index()\n",
    "        if first_idx is None: continue\n",
    "\n",
    "        df_target = df_model.loc[first_idx:].sort_index()\n",
    "        y = df_target[target_col]\n",
    "        X = df_target.drop(columns=[c for c in possible_disease_cols if c in df_target.columns], errors=\"ignore\")\n",
    "\n",
    "        mask_valid = y.notna() & ~X.isna().any(axis=1)\n",
    "        X = X[mask_valid]\n",
    "        y = y[mask_valid]\n",
    "\n",
    "        if len(X) < 10: continue\n",
    "\n",
    "        split_idx = int(len(X) * 0.75)\n",
    "        X_train, X_test = X.iloc[:split_idx], X.iloc[split_idx:]\n",
    "        y_train, y_test = y.iloc[:split_idx], y.iloc[split_idx:]\n",
    "\n",
    "        numeric_features = X_train.select_dtypes(include=[np.number]).columns.tolist()\n",
    "        categorical_features = X_train.select_dtypes(exclude=[np.number]).columns.tolist()\n",
    "\n",
    "        preprocess = ColumnTransformer([\n",
    "            (\"num\", \"passthrough\", numeric_features),\n",
    "            (\"cat\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False), categorical_features),\n",
    "        ], remainder=\"drop\")\n",
    "\n",
    "        model = Pipeline(steps=[\n",
    "            (\"preprocess\", preprocess),\n",
    "            (\"rf\", RandomForestRegressor(n_estimators=999, random_state=42, n_jobs=-1))\n",
    "        ])\n",
    "\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "\n",
    "        mae, mse, r2 = mean_absolute_error(y_test, y_pred), mean_squared_error(y_test, y_pred), r2_score(y_test, y_pred)\n",
    "        agg_series = calcular_importancia_agregada(model, numeric_features, categorical_features)\n",
    "\n",
    "        resultados.append({\n",
    "            \"modelo\": \"M2_temporal_NoSeason\",\n",
    "            \"dataset\": descricao_dataset,\n",
    "            \"doenca\": target_col,\n",
    "            \"MAE\": mae, \"MSE\": mse, \"R2\": r2\n",
    "        })\n",
    "\n",
    "        fazer_grafico_3plots(y_test, y_pred, agg_series, mae, mse, r2, target_col, descricao_dataset, \"(split temporal)\", output_dir / f\"RF_{target_col}_M2.png\")\n",
    "\n",
    "    return pd.DataFrame(resultados)\n",
    "\n",
    "# =====================================================\n",
    "# MODELO 3 – RF TEMPORAL + LAGS\n",
    "# =====================================================\n",
    "\n",
    "def rodar_rf_temporal_lag_para_df(df_input, output_folder_path, descricao_dataset):\n",
    "    df_base = df_input.copy()\n",
    "    df_base.index = pd.to_datetime(df_base.index)\n",
    "    df_base = df_base.sort_index()\n",
    "\n",
    "    # >>> AQUI REMOVEMOS SEASON E SEASON2 <<<\n",
    "    cols_to_drop = []\n",
    "    lista_remocao = [\"Data\", \"Ano\", \"Mes\", \"MesNum\", \"Mes_do_ano\", \"Season\", \"Season2\"]\n",
    "    \n",
    "    for c in lista_remocao:\n",
    "        if c in df_base.columns:\n",
    "            cols_to_drop.append(c)\n",
    "\n",
    "    extra_drop = [c for c in df_base.columns if c.endswith(\"_outlier_IQR\") or c.endswith(\"_extremo\")]\n",
    "    cols_to_drop.extend(extra_drop)\n",
    "\n",
    "    if cols_to_drop:\n",
    "        df_base = df_base.drop(columns=cols_to_drop, errors=\"ignore\")\n",
    "\n",
    "    output_dir = base_path / output_folder_path\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    resultados = []\n",
    "\n",
    "    for target_col in possible_disease_cols:\n",
    "        if target_col not in df_base.columns: continue\n",
    "        \n",
    "        first_idx = df_base[target_col].first_valid_index()\n",
    "        if first_idx is None: continue\n",
    "\n",
    "        df_target = df_base.loc[first_idx:].sort_index().copy()\n",
    "        \n",
    "        # Criar lags\n",
    "        df_lag = df_target.copy()\n",
    "        for lag in [1, 2, 3]:\n",
    "            for col in climate_cols:\n",
    "                if col in df_lag.columns:\n",
    "                    df_lag[f\"{col}_lag{lag}\"] = df_lag[col].shift(lag)\n",
    "        \n",
    "        lag_cols = [c for c in df_lag.columns if \"_lag\" in c]\n",
    "        if not lag_cols: continue\n",
    "        \n",
    "        df_lag = df_lag.dropna(subset=lag_cols)\n",
    "        if len(df_lag) < 10: continue\n",
    "\n",
    "        y = df_lag[target_col]\n",
    "        X = df_lag.drop(columns=[c for c in possible_disease_cols if c in df_lag.columns], errors=\"ignore\")\n",
    "\n",
    "        # Limpeza extra de colunas que podem ter sobrado\n",
    "        for c in lista_remocao:\n",
    "            if c in X.columns: X = X.drop(columns=[c])\n",
    "\n",
    "        mask_valid = y.notna() & ~X.isna().any(axis=1)\n",
    "        X, y = X[mask_valid], y[mask_valid]\n",
    "        if len(X) < 10: continue\n",
    "\n",
    "        split_idx = int(len(X) * 0.75)\n",
    "        X_train, X_test = X.iloc[:split_idx], X.iloc[split_idx:]\n",
    "        y_train, y_test = y.iloc[:split_idx], y.iloc[split_idx:]\n",
    "\n",
    "        numeric_features = X_train.select_dtypes(include=[np.number]).columns.tolist()\n",
    "        categorical_features = X_train.select_dtypes(exclude=[np.number]).columns.tolist()\n",
    "\n",
    "        preprocess = ColumnTransformer([\n",
    "            (\"num\", \"passthrough\", numeric_features),\n",
    "            (\"cat\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False), categorical_features),\n",
    "        ], remainder=\"drop\")\n",
    "\n",
    "        model = Pipeline(steps=[\n",
    "            (\"preprocess\", preprocess),\n",
    "            (\"rf\", RandomForestRegressor(n_estimators=999, random_state=42, n_jobs=-1))\n",
    "        ])\n",
    "\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "\n",
    "        mae, mse, r2 = mean_absolute_error(y_test, y_pred), mean_squared_error(y_test, y_pred), r2_score(y_test, y_pred)\n",
    "        agg_series = calcular_importancia_agregada(model, numeric_features, categorical_features)\n",
    "\n",
    "        resultados.append({\n",
    "            \"modelo\": \"M3_temporal_lag_NoSeason\",\n",
    "            \"dataset\": descricao_dataset,\n",
    "            \"doenca\": target_col,\n",
    "            \"MAE\": mae, \"MSE\": mse, \"R2\": r2\n",
    "        })\n",
    "\n",
    "        fazer_grafico_3plots(y_test, y_pred, agg_series, mae, mse, r2, target_col, descricao_dataset, \"(temporal+lags)\", output_dir / f\"RF_{target_col}_M3.png\")\n",
    "\n",
    "    return pd.DataFrame(resultados)\n",
    "\n",
    "# =====================================================\n",
    "# MODELO 3D – DOMADA\n",
    "# =====================================================\n",
    "\n",
    "def rodar_rf_temporal_lag_domada_para_df(df_input, output_folder_path, descricao_dataset):\n",
    "    df_base = df_input.copy()\n",
    "    df_base.index = pd.to_datetime(df_base.index)\n",
    "    df_base = df_base.sort_index()\n",
    "\n",
    "    # >>> AQUI REMOVEMOS SEASON E SEASON2 <<<\n",
    "    cols_to_drop = []\n",
    "    lista_remocao = [\"Data\", \"Ano\", \"Mes\", \"MesNum\", \"Mes_do_ano\", \"Season\", \"Season2\"]\n",
    "    \n",
    "    for c in lista_remocao:\n",
    "        if c in df_base.columns:\n",
    "            cols_to_drop.append(c)\n",
    "\n",
    "    extra_drop = [c for c in df_base.columns if c.endswith(\"_outlier_IQR\") or c.endswith(\"_extremo\")]\n",
    "    cols_to_drop.extend(extra_drop)\n",
    "\n",
    "    if cols_to_drop:\n",
    "        df_base = df_base.drop(columns=cols_to_drop, errors=\"ignore\")\n",
    "\n",
    "    output_dir = base_path / output_folder_path\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    resultados = []\n",
    "\n",
    "    for target_col in possible_disease_cols:\n",
    "        if target_col not in df_base.columns: continue\n",
    "        \n",
    "        first_idx = df_base[target_col].first_valid_index()\n",
    "        if first_idx is None: continue\n",
    "\n",
    "        df_target = df_base.loc[first_idx:].sort_index().copy()\n",
    "        \n",
    "        df_lag = df_target.copy()\n",
    "        for lag in [1, 2, 3]:\n",
    "            for col in climate_cols:\n",
    "                if col in df_lag.columns:\n",
    "                    df_lag[f\"{col}_lag{lag}\"] = df_lag[col].shift(lag)\n",
    "        \n",
    "        lag_cols = [c for c in df_lag.columns if \"_lag\" in c]\n",
    "        if not lag_cols: continue\n",
    "        \n",
    "        df_lag = df_lag.dropna(subset=lag_cols)\n",
    "        if len(df_lag) < 10: continue\n",
    "\n",
    "        y = df_lag[target_col]\n",
    "        X = df_lag.drop(columns=[c for c in possible_disease_cols if c in df_lag.columns], errors=\"ignore\")\n",
    "\n",
    "        for c in lista_remocao:\n",
    "            if c in X.columns: X = X.drop(columns=[c])\n",
    "\n",
    "        mask_valid = y.notna() & ~X.isna().any(axis=1)\n",
    "        X, y = X[mask_valid], y[mask_valid]\n",
    "        if len(X) < 10: continue\n",
    "\n",
    "        split_idx = int(len(X) * 0.75)\n",
    "        X_train, X_test = X.iloc[:split_idx], X.iloc[split_idx:]\n",
    "        y_train, y_test = y.iloc[:split_idx], y.iloc[split_idx:]\n",
    "\n",
    "        numeric_features = X_train.select_dtypes(include=[np.number]).columns.tolist()\n",
    "        categorical_features = X_train.select_dtypes(exclude=[np.number]).columns.tolist()\n",
    "\n",
    "        preprocess = ColumnTransformer([\n",
    "            (\"num\", \"passthrough\", numeric_features),\n",
    "            (\"cat\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False), categorical_features),\n",
    "        ], remainder=\"drop\")\n",
    "\n",
    "        model = Pipeline(steps=[\n",
    "            (\"preprocess\", preprocess),\n",
    "            (\"rf\", RandomForestRegressor(n_estimators=500, max_depth=5, min_samples_leaf=3, max_features=\"sqrt\", random_state=42, n_jobs=-1))\n",
    "        ])\n",
    "\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "\n",
    "        mae, mse, r2 = mean_absolute_error(y_test, y_pred), mean_squared_error(y_test, y_pred), r2_score(y_test, y_pred)\n",
    "        agg_series = calcular_importancia_agregada(model, numeric_features, categorical_features)\n",
    "\n",
    "        resultados.append({\n",
    "            \"modelo\": \"M3D_domada_NoSeason\",\n",
    "            \"dataset\": descricao_dataset,\n",
    "            \"doenca\": target_col,\n",
    "            \"MAE\": mae, \"MSE\": mse, \"R2\": r2\n",
    "        })\n",
    "\n",
    "        fazer_grafico_3plots(y_test, y_pred, agg_series, mae, mse, r2, target_col, descricao_dataset, \"(domada)\", output_dir / f\"RF_{target_col}_M3D.png\")\n",
    "\n",
    "    return pd.DataFrame(resultados)\n",
    "\n",
    "# =====================================================\n",
    "# RODAR TUDO (COM PASTA MÃE E SEM SEASON)\n",
    "# =====================================================\n",
    "\n",
    "# Dicionário de bases (Assumindo que os DataFrames já existem na memória)\n",
    "bases = {\n",
    "    \"base\": df,\n",
    "    \"IQR\": df_iqr_sem_outliers,\n",
    "    \"p99\": df_p99_sem_extremos\n",
    "}\n",
    "\n",
    "resultados_list = []\n",
    "\n",
    "# Caminho relativo para a pasta mãe dentro do base_path\n",
    "path_mae = Path(pasta_mae_nome)\n",
    "\n",
    "for desc, df_base_modelo in bases.items():\n",
    "    print(f\"\\n>>> Processando Dataset: {desc} (No Season) <<<\")\n",
    "    \n",
    "    # M1\n",
    "    p1 = path_mae / f\"RF_M1_{desc}_NoSeason\"\n",
    "    resultados_list.append(rodar_rf_basico_para_df(df_base_modelo, p1, desc))\n",
    "    \n",
    "    # M2\n",
    "    p2 = path_mae / f\"RF_M2_{desc}_temporal_NoSeason\"\n",
    "    resultados_list.append(rodar_rf_temporal_para_df(df_base_modelo, p2, desc))\n",
    "    \n",
    "    # M3\n",
    "    p3 = path_mae / f\"RF_M3_{desc}_lag_NoSeason\"\n",
    "    resultados_list.append(rodar_rf_temporal_lag_para_df(df_base_modelo, p3, desc))\n",
    "    \n",
    "    # M3D\n",
    "    p3d = path_mae / f\"RF_M3D_{desc}_domada_NoSeason\"\n",
    "    resultados_list.append(rodar_rf_temporal_lag_domada_para_df(df_base_modelo, p3d, desc))\n",
    "\n",
    "# Concatenar tudo\n",
    "resumo_geral = pd.concat(resultados_list, ignore_index=True)\n",
    "\n",
    "print(\"\\n=== RESUMO GERAL DE TODOS OS MODELOS (NO SEASON) ===\")\n",
    "print(resumo_geral.head())\n",
    "\n",
    "# =====================================================\n",
    "# SALVAR EXCEL NA PASTA MÃE\n",
    "# =====================================================\n",
    "\n",
    "excel_dir = base_path / pasta_mae_nome\n",
    "excel_dir.mkdir(parents=True, exist_ok=True)\n",
    "excel_path = excel_dir / \"Tabela_Final_RF_Modelos_NoSeason.xlsx\"\n",
    "\n",
    "with pd.ExcelWriter(excel_path, engine=\"openpyxl\") as writer:\n",
    "    for doenca in possible_disease_cols:\n",
    "        df_doenca = resumo_geral[resumo_geral[\"doenca\"] == doenca].copy()\n",
    "        if df_doenca.empty: continue\n",
    "        \n",
    "        # Reordenar colunas se possível\n",
    "        desired_cols = [\"modelo\", \"dataset\", \"doenca\", \"MAE\", \"MSE\", \"R2\"]\n",
    "        final_cols = [c for c in desired_cols if c in df_doenca.columns]\n",
    "        df_doenca = df_doenca[final_cols]\n",
    "\n",
    "        sheet_name = doenca[:31]\n",
    "        df_doenca.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "    \n",
    "    resumo_geral.to_excel(writer, sheet_name=\"ResumoGeral\", index=False)\n",
    "\n",
    "print(f\"\\nArquivo Excel final salvo em: {excel_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c0ed045",
   "metadata": {},
   "source": [
    "## Classificando mês epidêmico vs Não epidêmico "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0b0d1deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    roc_auc_score,\n",
    "    confusion_matrix,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    roc_curve,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1a9b4d32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== [Modelo4-NoSeason] Doença alvo: Dengue ===\n",
      "Primeira data usada: 2014-01-01 00:00:00\n",
      "Limiar (p75) para Dengue: 33840.00 casos\n",
      "Dengue: 33 meses epidêmicos (1) de 129 meses totais (25.6%)\n",
      "Melhor limiar (Youden): 0.10 | Sens: 0.92 | Espec: 0.85\n",
      "\n",
      "=== [Modelo4-NoSeason] Doença alvo: Zika ===\n",
      "Primeira data usada: 2016-01-01 00:00:00\n",
      "Limiar (p75) para Zika: 170.00 casos\n",
      "Zika: 27 meses epidêmicos (1) de 105 meses totais (25.7%)\n",
      "Melhor limiar (Youden): 0.30 | Sens: 1.00 | Espec: 0.77\n",
      "\n",
      "=== [Modelo4-NoSeason] Doença alvo: Chik ===\n",
      "Primeira data usada: 2017-01-01 00:00:00\n",
      "Limiar (p75) para Chik: 912.00 casos\n",
      "Chik: 24 meses epidêmicos (1) de 93 meses totais (25.8%)\n",
      "Melhor limiar (Youden): 0.20 | Sens: 0.47 | Espec: 1.00\n",
      "\n",
      "=== [Modelo4-NoSeason] Doença alvo: LeishVis ===\n",
      "Primeira data usada: 2014-01-01 00:00:00\n",
      "Limiar (p75) para LeishVis: 14.00 casos\n",
      "LeishVis: 34 meses epidêmicos (1) de 129 meses totais (26.4%)\n",
      "Melhor limiar (Youden): 0.30 | Sens: 0.67 | Espec: 0.50\n",
      "\n",
      "=== [Modelo4-NoSeason] Doença alvo: LeishTeg ===\n",
      "Primeira data usada: 2014-01-01 00:00:00\n",
      "Limiar (p75) para LeishTeg: 34.00 casos\n",
      "LeishTeg: 33 meses epidêmicos (1) de 129 meses totais (25.6%)\n",
      "Melhor limiar (Youden): 0.20 | Sens: 1.00 | Espec: 0.36\n",
      "\n",
      "=== [Modelo4-NoSeason] Doença alvo: LeishT ===\n",
      "Primeira data usada: 2014-01-01 00:00:00\n",
      "Limiar (p75) para LeishT: 45.00 casos\n",
      "LeishT: 34 meses epidêmicos (1) de 129 meses totais (26.4%)\n",
      "Melhor limiar (Youden): 0.30 | Sens: 1.00 | Espec: 0.61\n",
      "\n",
      "=== RESUMO GERAL CLASSIFICAÇÃO EPIDÊMICO vs NÃO (NO SEASON) ===\n",
      "     doenca       AUC  best_threshold  accuracy_best  precision_best  \\\n",
      "0    Dengue  0.953846             0.1       0.878788        0.800000   \n",
      "1      Zika  0.918182             0.3       0.814815        0.500000   \n",
      "2      Chik  0.731092             0.2       0.625000        1.000000   \n",
      "3  LeishVis  0.388889             0.3       0.515152        0.117647   \n",
      "4  LeishTeg  0.707143             0.2       0.454545        0.217391   \n",
      "5    LeishT  0.822581             0.3       0.636364        0.142857   \n",
      "\n",
      "   recall_sensibilidade_best  especificidade_best   f1_best  \n",
      "0                   0.923077             0.850000  0.857143  \n",
      "1                   1.000000             0.772727  0.666667  \n",
      "2                   0.470588             1.000000  0.640000  \n",
      "3                   0.666667             0.500000  0.200000  \n",
      "4                   1.000000             0.357143  0.357143  \n",
      "5                   1.000000             0.612903  0.250000  \n",
      "\n",
      "Arquivo Excel salvo em: C:\\Users\\idolo\\OneDrive\\Área de Trabalho\\Curso - Modelos Computacionais\\Projeto\\RF_Modelo4_Classificacao_Epidemico_NoSeason\\Tabela_Modelo4_Classificacao_Epidemico_NoSeason.xlsx\n",
      "Figuras salvas em: C:\\Users\\idolo\\OneDrive\\Área de Trabalho\\Curso - Modelos Computacionais\\Projeto\\RF_Modelo4_Classificacao_Epidemico_NoSeason\n",
      "\n",
      "=== [Modelo4] Doença alvo: Dengue ===\n",
      "Primeira data usada: 2014-01-01 00:00:00\n",
      "Limiar (p75) para Dengue: 33840.00 casos\n",
      "Dengue: 33 meses epidêmicos (1) de 129 meses totais (25.6% positivos)\n",
      "Treino: 96 meses, Teste: 33 meses\n",
      "Melhor limiar para Dengue (Youden): 0.10\n",
      "  Sensibilidade: 0.923  Especificidade: 0.850  Acurácia: 0.879\n",
      "\n",
      "=== [Modelo4] Doença alvo: Zika ===\n",
      "Primeira data usada: 2016-01-01 00:00:00\n",
      "Limiar (p75) para Zika: 170.00 casos\n",
      "Zika: 27 meses epidêmicos (1) de 105 meses totais (25.7% positivos)\n",
      "Treino: 78 meses, Teste: 27 meses\n",
      "Melhor limiar para Zika (Youden): 0.30\n",
      "  Sensibilidade: 1.000  Especificidade: 0.727  Acurácia: 0.778\n",
      "\n",
      "=== [Modelo4] Doença alvo: Chik ===\n",
      "Primeira data usada: 2017-01-01 00:00:00\n",
      "Limiar (p75) para Chik: 912.00 casos\n",
      "Chik: 24 meses epidêmicos (1) de 93 meses totais (25.8% positivos)\n",
      "Treino: 69 meses, Teste: 24 meses\n",
      "Melhor limiar para Chik (Youden): 0.20\n",
      "  Sensibilidade: 0.471  Especificidade: 1.000  Acurácia: 0.625\n",
      "\n",
      "=== [Modelo4] Doença alvo: LeishVis ===\n",
      "Primeira data usada: 2014-01-01 00:00:00\n",
      "Limiar (p75) para LeishVis: 14.00 casos\n",
      "LeishVis: 34 meses epidêmicos (1) de 129 meses totais (26.4% positivos)\n",
      "Treino: 96 meses, Teste: 33 meses\n",
      "Melhor limiar para LeishVis (Youden): 0.10\n",
      "  Sensibilidade: 1.000  Especificidade: 0.033  Acurácia: 0.121\n",
      "\n",
      "=== [Modelo4] Doença alvo: LeishTeg ===\n",
      "Primeira data usada: 2014-01-01 00:00:00\n",
      "Limiar (p75) para LeishTeg: 34.00 casos\n",
      "LeishTeg: 33 meses epidêmicos (1) de 129 meses totais (25.6% positivos)\n",
      "Treino: 96 meses, Teste: 33 meses\n",
      "Melhor limiar para LeishTeg (Youden): 0.20\n",
      "  Sensibilidade: 1.000  Especificidade: 0.357  Acurácia: 0.455\n",
      "\n",
      "=== [Modelo4] Doença alvo: LeishT ===\n",
      "Primeira data usada: 2014-01-01 00:00:00\n",
      "Limiar (p75) para LeishT: 45.00 casos\n",
      "LeishT: 34 meses epidêmicos (1) de 129 meses totais (26.4% positivos)\n",
      "Treino: 96 meses, Teste: 33 meses\n",
      "Melhor limiar para LeishT (Youden): 0.30\n",
      "  Sensibilidade: 1.000  Especificidade: 0.677  Acurácia: 0.697\n",
      "\n",
      "=== RESUMO GERAL CLASSIFICAÇÃO EPIDÊMICO vs NÃO ===\n",
      "     doenca       AUC  best_threshold  accuracy_best  precision_best  \\\n",
      "0    Dengue  0.957692             0.1       0.878788        0.800000   \n",
      "1      Zika  0.900000             0.3       0.777778        0.454545   \n",
      "2      Chik  0.747899             0.2       0.625000        1.000000   \n",
      "3  LeishVis  0.388889             0.1       0.121212        0.093750   \n",
      "4  LeishTeg  0.721429             0.2       0.454545        0.217391   \n",
      "5    LeishT  0.822581             0.3       0.696970        0.166667   \n",
      "\n",
      "   recall_sensibilidade_best  especificidade_best   f1_best  n_treino  \\\n",
      "0                   0.923077             0.850000  0.857143        96   \n",
      "1                   1.000000             0.727273  0.625000        78   \n",
      "2                   0.470588             1.000000  0.640000        69   \n",
      "3                   1.000000             0.033333  0.171429        96   \n",
      "4                   1.000000             0.357143  0.357143        96   \n",
      "5                   1.000000             0.677419  0.285714        96   \n",
      "\n",
      "   n_teste  n_pos_treino  n_pos_teste  limiar_casos_percentil  \\\n",
      "0       33            20           13                    0.75   \n",
      "1       27            22            5                    0.75   \n",
      "2       24             7           17                    0.75   \n",
      "3       33            31            3                    0.75   \n",
      "4       33            28            5                    0.75   \n",
      "5       33            32            2                    0.75   \n",
      "\n",
      "   valor_limiar_casos  \n",
      "0             33840.0  \n",
      "1               170.0  \n",
      "2               912.0  \n",
      "3                14.0  \n",
      "4                34.0  \n",
      "5                45.0  \n",
      "\n",
      "Arquivo Excel de classificação salvo em: C:\\Users\\idolo\\OneDrive\\Área de Trabalho\\Curso - Modelos Computacionais\\Projeto\\RF_Modelo4_Classificacao_Epidemico\\Tabela_Modelo4_Classificacao_Epidemico.xlsx\n",
      "Figuras (ROC e Matriz de Confusão) salvas em: C:\\Users\\idolo\\OneDrive\\Área de Trabalho\\Curso - Modelos Computacionais\\Projeto\\RF_Modelo4_Classificacao_Epidemico\n"
     ]
    }
   ],
   "source": [
    "# ==============================================\n",
    "# MODELO 4 – CLASSIFICAÇÃO: MÊS EPIDÊMICO vs NÃO (NO SEASON)\n",
    "# ==============================================\n",
    "\n",
    "# Caminho base\n",
    "base_path = Path(r\"C:\\Users\\idolo\\OneDrive\\Área de Trabalho\\Curso - Modelos Computacionais\\Projeto\")\n",
    "\n",
    "# Lista de doenças e variáveis climáticas\n",
    "possible_disease_cols = [\"Dengue\", \"Zika\", \"Chik\", \"LeishVis\", \"LeishTeg\", \"LeishT\"]\n",
    "climate_cols = [\"Precipt\", \"AvgTemp\", \"MaxTemp\", \"MinTemp\", \"AvgHumid\", \"AvgWin\"]\n",
    "\n",
    "# Percentil para definir \"mês epidêmico\"\n",
    "epidemic_percentile = 0.75  # p75\n",
    "\n",
    "# Pasta de saída (Atualizada para _NoSeason)\n",
    "output_dir_class = base_path / \"RF_Modelo4_Classificacao_Epidemico_NoSeason\"\n",
    "output_dir_class.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "def classificar_meses_epidemicos(\n",
    "    df_input,\n",
    "    target_cols=possible_disease_cols,\n",
    "    percentile_limiar=epidemic_percentile,\n",
    "    max_lag=3,\n",
    "    thresholds_list=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Roda RandomForestClassifier para prever se um mês é epidêmico (1) ou não (0).\n",
    "    VERSÃO NO SEASON: Remove explicitamente Season e Season2.\n",
    "    \"\"\"\n",
    "\n",
    "    if thresholds_list is None:\n",
    "        thresholds_list = [round(t, 2) for t in np.arange(0.1, 1.0, 0.1)]\n",
    "\n",
    "    # cópia de trabalho\n",
    "    df_base = df_input.copy()\n",
    "    df_base.index = pd.to_datetime(df_base.index)\n",
    "    df_base = df_base.sort_index()\n",
    "\n",
    "    # >>> REMOÇÃO DE COLUNAS (Incluindo Season/Season2) <<<\n",
    "    cols_to_drop = []\n",
    "    lista_remocao = [\"Data\", \"Ano\", \"Mes\", \"MesNum\", \"Mes_do_ano\", \"Season\", \"Season2\"]\n",
    "    \n",
    "    for c in lista_remocao:\n",
    "        if c in df_base.columns:\n",
    "            cols_to_drop.append(c)\n",
    "\n",
    "    # se vierem flags de outlier por engano, também tira\n",
    "    extra_drop = [c for c in df_base.columns if c.endswith(\"_outlier_IQR\") or c.endswith(\"_extremo\")]\n",
    "    cols_to_drop.extend(extra_drop)\n",
    "\n",
    "    if cols_to_drop:\n",
    "        df_base = df_base.drop(columns=cols_to_drop, errors=\"ignore\")\n",
    "\n",
    "    resumo_linhas = []\n",
    "    thresholds_linhas = []\n",
    "\n",
    "    # Loop por doença\n",
    "    for target_col in target_cols:\n",
    "        if target_col not in df_base.columns:\n",
    "            print(f\"[AVISO] {target_col} não está em df. Pulando.\")\n",
    "            continue\n",
    "\n",
    "        # primeira data com valor não-nulo para essa doença\n",
    "        first_idx = df_base[target_col].first_valid_index()\n",
    "        if first_idx is None:\n",
    "            print(f\"[AVISO] {target_col} é toda NaN. Pulando.\")\n",
    "            continue\n",
    "\n",
    "        print(f\"\\n=== [Modelo4-NoSeason] Doença alvo: {target_col} ===\")\n",
    "        print(f\"Primeira data usada: {first_idx}\")\n",
    "\n",
    "        # Subdataframe a partir do primeiro valor válido\n",
    "        df_target = df_base.loc[first_idx:].copy()\n",
    "        df_target = df_target.sort_index()\n",
    "\n",
    "        # criar mês do ano (1..12) a partir do índice e codificação cíclica\n",
    "        df_target[\"Mes_do_ano_temp\"] = df_target.index.month\n",
    "        df_target[\"Mes_sin\"] = np.sin(2 * np.pi * (df_target[\"Mes_do_ano_temp\"] - 1) / 12.0)\n",
    "        df_target[\"Mes_cos\"] = np.cos(2 * np.pi * (df_target[\"Mes_do_ano_temp\"] - 1) / 12.0)\n",
    "        df_target = df_target.drop(columns=[\"Mes_do_ano_temp\"])\n",
    "\n",
    "        # criar lags das variáveis climáticas que existirem\n",
    "        for lag in range(1, max_lag + 1):\n",
    "            for col in climate_cols:\n",
    "                if col in df_target.columns:\n",
    "                    df_target[f\"{col}_lag{lag}\"] = df_target[col].shift(lag)\n",
    "\n",
    "        # variável contínua de casos (para definir o limiar)\n",
    "        y_cont = df_target[target_col]\n",
    "\n",
    "        # Definir colunas de X (removendo doenças)\n",
    "        colunas_possiveis_X = list(df_target.columns)\n",
    "        for dcol in possible_disease_cols:\n",
    "            if dcol in colunas_possiveis_X:\n",
    "                colunas_possiveis_X.remove(dcol)\n",
    "\n",
    "        X_pre = df_target[colunas_possiveis_X].copy()\n",
    "\n",
    "        # Remover linhas com NaN em qualquer feature (por causa dos lags)\n",
    "        mask_valid = (~X_pre.isna().any(axis=1)) & y_cont.notna()\n",
    "        X_pre = X_pre[mask_valid]\n",
    "        y_cont_use = y_cont[mask_valid]\n",
    "\n",
    "        if len(X_pre) < 20:\n",
    "            print(f\"[AVISO] Muito poucos dados válidos para {target_col} após lags/limpeza. Pulando.\")\n",
    "            continue\n",
    "\n",
    "        # Definir limiar de \"mês epidêmico\"\n",
    "        limiar = y_cont_use.quantile(percentile_limiar)\n",
    "        print(f\"Limiar (p{int(percentile_limiar*100)}) para {target_col}: {limiar:.2f} casos\")\n",
    "\n",
    "        # y binário\n",
    "        y_bin = (y_cont_use >= limiar).astype(int)\n",
    "\n",
    "        # X definitivo (garantia extra de limpeza)\n",
    "        X = X_pre.copy()\n",
    "        for c in lista_remocao: # Garante que Season não sobrou\n",
    "            if c in X.columns: X = X.drop(columns=[c])\n",
    "            \n",
    "        X = X.drop(columns=[c for c in possible_disease_cols if c in X.columns], errors=\"ignore\")\n",
    "\n",
    "        n_pos = int(y_bin.sum())\n",
    "        n_total = len(y_bin)\n",
    "        print(f\"{target_col}: {n_pos} meses epidêmicos (1) de {n_total} meses totais ({n_pos/n_total:.1%})\")\n",
    "\n",
    "        # SPLIT TEMPORAL (75% / 25%)\n",
    "        n = len(X)\n",
    "        split_idx = int(n * 0.75)\n",
    "\n",
    "        X_train, X_test = X.iloc[:split_idx], X.iloc[split_idx:]\n",
    "        y_train, y_test = y_bin.iloc[:split_idx], y_bin.iloc[split_idx:]\n",
    "\n",
    "        if y_train.nunique() < 2 or y_test.nunique() < 2:\n",
    "            print(f\"[AVISO] Conjunto de treino ou teste de {target_col} só tem uma classe. Pulando.\")\n",
    "            continue\n",
    "\n",
    "        # Detectar tipos de colunas\n",
    "        numeric_features = X_train.select_dtypes(include=[np.number]).columns.tolist()\n",
    "        categorical_features = X_train.select_dtypes(exclude=[np.number]).columns.tolist()\n",
    "\n",
    "        preprocess = ColumnTransformer([\n",
    "            (\"num\", \"passthrough\", numeric_features),\n",
    "            (\"cat\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False), categorical_features),\n",
    "        ], remainder=\"drop\")\n",
    "\n",
    "        # Random Forest Classifier \"domada\"\n",
    "        model = Pipeline(steps=[\n",
    "            (\"preprocess\", preprocess),\n",
    "            (\"rf\", RandomForestClassifier(\n",
    "                n_estimators=500,\n",
    "                max_depth=5,\n",
    "                min_samples_leaf=3,\n",
    "                max_features=\"sqrt\",\n",
    "                random_state=42,\n",
    "                n_jobs=-1\n",
    "            ))\n",
    "        ])\n",
    "\n",
    "        model.fit(X_train, y_train)\n",
    "        y_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "        # AUC\n",
    "        try:\n",
    "            auc = roc_auc_score(y_test, y_proba)\n",
    "        except ValueError:\n",
    "            auc = np.nan\n",
    "\n",
    "        # ROC curve plot\n",
    "        fpr, tpr, roc_thresholds = roc_curve(y_test, y_proba)\n",
    "        fig_roc, ax_roc = plt.subplots(figsize=(6, 5))\n",
    "        ax_roc.plot(fpr, tpr, label=f\"ROC (AUC = {auc:.3f})\")\n",
    "        ax_roc.plot([0, 1], [0, 1], linestyle=\"--\", label=\"Aleatório\")\n",
    "        ax_roc.set_xlabel(\"Falso Positivo (1 - Especificidade)\")\n",
    "        ax_roc.set_ylabel(\"Verdadeiro Positivo (Sensibilidade)\")\n",
    "        ax_roc.set_title(f\"Curva ROC (NoSeason) — {target_col}\")\n",
    "        ax_roc.legend(loc=\"lower right\")\n",
    "        fig_roc.tight_layout()\n",
    "        fig_roc.savefig(output_dir_class / f\"ROC_{target_col}.png\", dpi=300, bbox_inches=\"tight\")\n",
    "        plt.close(fig_roc)\n",
    "\n",
    "        # VARREDURA DE THRESHOLDS\n",
    "        best_thr = None\n",
    "        best_youden = -np.inf\n",
    "        best_metrics = None\n",
    "\n",
    "        for thr in thresholds_list:\n",
    "            y_pred_thr = (y_proba >= thr).astype(int)\n",
    "            \n",
    "            acc = accuracy_score(y_test, y_pred_thr)\n",
    "            prec = precision_score(y_test, y_pred_thr, zero_division=0)\n",
    "            rec = recall_score(y_test, y_pred_thr, zero_division=0)\n",
    "            f1 = f1_score(y_test, y_pred_thr, zero_division=0)\n",
    "            \n",
    "            tn, fp, fn, tp = confusion_matrix(y_test, y_pred_thr, labels=[0, 1]).ravel()\n",
    "            sens = rec\n",
    "            espec = tn / (tn + fp) if (tn + fp) > 0 else np.nan\n",
    "            youden = sens + espec - 1 if np.isfinite(sens) and np.isfinite(espec) else -np.inf\n",
    "\n",
    "            thresholds_linhas.append({\n",
    "                \"doenca\": target_col,\n",
    "                \"threshold\": thr,\n",
    "                \"AUC\": auc,\n",
    "                \"accuracy\": acc,\n",
    "                \"precision\": prec,\n",
    "                \"recall_sensibilidade\": sens,\n",
    "                \"especificidade\": espec,\n",
    "                \"f1\": f1,\n",
    "                \"youden\": youden,\n",
    "                \"n_treino\": len(X_train),\n",
    "                \"n_teste\": len(X_test)\n",
    "            })\n",
    "\n",
    "            if youden > best_youden:\n",
    "                best_youden = youden\n",
    "                best_thr = thr\n",
    "                best_metrics = {\n",
    "                    \"accuracy\": acc, \"precision\": prec, \n",
    "                    \"recall_sensibilidade\": sens, \"especificidade\": espec, \"f1\": f1\n",
    "                }\n",
    "\n",
    "        if best_thr is None: continue\n",
    "\n",
    "        print(f\"Melhor limiar (Youden): {best_thr:.2f} | Sens: {best_metrics['recall_sensibilidade']:.2f} | Espec: {best_metrics['especificidade']:.2f}\")\n",
    "\n",
    "        # Matriz de confusão no melhor threshold\n",
    "        y_pred_best = (y_proba >= best_thr).astype(int)\n",
    "        cm = confusion_matrix(y_test, y_pred_best, labels=[0, 1])\n",
    "        cm_df = pd.DataFrame(cm, index=[\"Real 0\", \"Real 1\"], columns=[\"Prev 0\", \"Prev 1\"])\n",
    "\n",
    "        fig_cm, ax_cm = plt.subplots(figsize=(6, 5))\n",
    "        sns.heatmap(cm_df, annot=True, fmt=\"d\", ax=ax_cm)\n",
    "        ax_cm.set_title(f\"Matriz de Confusão (NoSeason) — {target_col}\\nThreshold = {best_thr:.2f}\")\n",
    "        fig_cm.tight_layout()\n",
    "        fig_cm.savefig(output_dir_class / f\"CM_{target_col}_best.png\", dpi=300, bbox_inches=\"tight\")\n",
    "        plt.close(fig_cm)\n",
    "\n",
    "        resumo_linhas.append({\n",
    "            \"doenca\": target_col,\n",
    "            \"AUC\": auc,\n",
    "            \"best_threshold\": best_thr,\n",
    "            \"accuracy_best\": best_metrics[\"accuracy\"],\n",
    "            \"precision_best\": best_metrics[\"precision\"],\n",
    "            \"recall_sensibilidade_best\": best_metrics[\"recall_sensibilidade\"],\n",
    "            \"especificidade_best\": best_metrics[\"especificidade\"],\n",
    "            \"f1_best\": best_metrics[\"f1\"]\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(resumo_linhas), pd.DataFrame(thresholds_linhas)\n",
    "\n",
    "\n",
    "# =======================\n",
    "# Rodar Modelo 4 em df (No Season)\n",
    "# =======================\n",
    "\n",
    "# Assumindo que 'df' já existe na memória com os dados originais\n",
    "resumo_epidemico, thresholds_epidemico = classificar_meses_epidemicos(df)\n",
    "\n",
    "print(\"\\n=== RESUMO GERAL CLASSIFICAÇÃO EPIDÊMICO vs NÃO (NO SEASON) ===\")\n",
    "print(resumo_epidemico)\n",
    "\n",
    "\n",
    "# ==============================================\n",
    "# Salvar tabelas em Excel\n",
    "# ==============================================\n",
    "\n",
    "excel_path = output_dir_class / \"Tabela_Modelo4_Classificacao_Epidemico_NoSeason.xlsx\"\n",
    "\n",
    "with pd.ExcelWriter(excel_path, engine=\"openpyxl\") as writer:\n",
    "    if not resumo_epidemico.empty:\n",
    "        resumo_epidemico.to_excel(writer, sheet_name=\"Resumo\", index=False)\n",
    "\n",
    "    for doenca in possible_disease_cols:\n",
    "        df_thr_doenca = thresholds_epidemico[thresholds_epidemico[\"doenca\"] == doenca].copy()\n",
    "        if df_thr_doenca.empty: continue\n",
    "\n",
    "        sheet_name = doenca[:31]\n",
    "        df_thr_doenca.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "\n",
    "print(f\"\\nArquivo Excel salvo em: {excel_path}\")\n",
    "print(f\"Figuras salvas em: {output_dir_class}\")\n",
    "climate_cols = [\"Precipt\", \"AvgTemp\", \"MaxTemp\", \"MinTemp\", \"AvgHumid\", \"AvgWin\"]  # só usa as que existirem\n",
    "\n",
    "# Percentil para definir \"mês epidêmico\"\n",
    "epidemic_percentile = 0.75  # p75\n",
    "\n",
    "# Pasta de saída para tudo desse modelo de classificação\n",
    "output_dir_class = base_path / \"RF_Modelo4_Classificacao_Epidemico\"\n",
    "output_dir_class.mkdir(exist_ok=True)\n",
    "\n",
    "\n",
    "def classificar_meses_epidemicos(\n",
    "    df_input,\n",
    "    target_cols=possible_disease_cols,\n",
    "    percentile_limiar=epidemic_percentile,\n",
    "    max_lag=3,\n",
    "    thresholds_list=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Roda RandomForestClassifier para prever se um mês é epidêmico (1) ou não (0),\n",
    "    para cada doença em target_cols, usando:\n",
    "      - base df_input\n",
    "      - lags (1..max_lag) das variáveis climáticas disponíveis\n",
    "      - mês em sin/cos (codificação cíclica)\n",
    "      - split temporal (75% / 25%)\n",
    "      - threshold sweep (ex.: 0.1, 0.2, ..., 0.9)\n",
    "    Retorna:\n",
    "      - resumo_df: uma linha por doença (AUC, melhor threshold, métricas no melhor threshold)\n",
    "      - thresholds_all_df: tabela com métricas por doença x threshold\n",
    "    \"\"\"\n",
    "\n",
    "    if thresholds_list is None:\n",
    "        thresholds_list = [round(t, 2) for t in np.arange(0.1, 1.0, 0.1)]  # 0.1, 0.2, ..., 0.9\n",
    "\n",
    "    # cópia de trabalho\n",
    "    df_base = df_input.copy()\n",
    "    df_base.index = pd.to_datetime(df_base.index)\n",
    "    df_base = df_base.sort_index()\n",
    "\n",
    "    # garantir que não tem colunas estranhas de tempo\n",
    "    cols_to_drop = []\n",
    "    for c in [\"Data\", \"Ano\", \"Mes\", \"MesNum\", \"Mes_do_ano\"]:\n",
    "        if c in df_base.columns:\n",
    "            cols_to_drop.append(c)\n",
    "\n",
    "    # se vierem flags de outlier por engano, também tira\n",
    "    extra_drop = [c for c in df_base.columns\n",
    "                  if c.endswith(\"_outlier_IQR\") or c.endswith(\"_extremo\")]\n",
    "    cols_to_drop.extend(extra_drop)\n",
    "\n",
    "    if cols_to_drop:\n",
    "        df_base = df_base.drop(columns=cols_to_drop, errors=\"ignore\")\n",
    "\n",
    "    resumo_linhas = []\n",
    "    thresholds_linhas = []\n",
    "\n",
    "    # Loop por doença\n",
    "    for target_col in target_cols:\n",
    "        if target_col not in df_base.columns:\n",
    "            print(f\"[AVISO] {target_col} não está em df. Pulando.\")\n",
    "            continue\n",
    "\n",
    "        # primeira data com valor não-nulo para essa doença\n",
    "        first_idx = df_base[target_col].first_valid_index()\n",
    "        if first_idx is None:\n",
    "            print(f\"[AVISO] {target_col} é toda NaN. Pulando.\")\n",
    "            continue\n",
    "\n",
    "        print(f\"\\n=== [Modelo4] Doença alvo: {target_col} ===\")\n",
    "        print(f\"Primeira data usada: {first_idx}\")\n",
    "\n",
    "        # Subdataframe a partir do primeiro valor válido\n",
    "        df_target = df_base.loc[first_idx:].copy()\n",
    "        df_target = df_target.sort_index()\n",
    "\n",
    "        # criar mês do ano (1..12) a partir do índice e codificação cíclica\n",
    "        df_target[\"Mes_do_ano\"] = df_target.index.month\n",
    "        df_target[\"Mes_sin\"] = np.sin(2 * np.pi * (df_target[\"Mes_do_ano\"] - 1) / 12.0)\n",
    "        df_target[\"Mes_cos\"] = np.cos(2 * np.pi * (df_target[\"Mes_do_ano\"] - 1) / 12.0)\n",
    "\n",
    "        # criar lags das variáveis climáticas que existirem\n",
    "        for lag in range(1, max_lag + 1):\n",
    "            for col in climate_cols:\n",
    "                if col in df_target.columns:\n",
    "                    df_target[f\"{col}_lag{lag}\"] = df_target[col].shift(lag)\n",
    "\n",
    "        # variável contínua de casos (para definir o limiar)\n",
    "        y_cont = df_target[target_col]\n",
    "\n",
    "        # remover as primeiras linhas que ficaram com NaN nos lags (onde necessário)\n",
    "        # vamos considerar como features todas as colunas climáticas + seus lags + Mes_sin/cos\n",
    "        # Primeiro, definimos quais colunas potenciais de X (sem tirar doenças ainda)\n",
    "        colunas_possiveis_X = list(df_target.columns)\n",
    "\n",
    "        # mas vamos tirar as próprias doenças (não podem ser preditoras)\n",
    "        for dcol in possible_disease_cols:\n",
    "            if dcol in colunas_possiveis_X:\n",
    "                colunas_possiveis_X.remove(dcol)\n",
    "\n",
    "        # vamos construir X preliminar\n",
    "        X_pre = df_target[colunas_possiveis_X].copy()\n",
    "\n",
    "        # Remover linhas com NaN em qualquer feature (principalmente por causa dos lags)\n",
    "        # e também garantir que y_cont não seja NaN nessas linhas\n",
    "        mask_valid = (~X_pre.isna().any(axis=1)) & y_cont.notna()\n",
    "        X_pre = X_pre[mask_valid]\n",
    "        y_cont_use = y_cont[mask_valid]\n",
    "\n",
    "        if len(X_pre) < 20:\n",
    "            print(f\"[AVISO] Muito poucos dados válidos para {target_col} após lags/limpeza. Pulando.\")\n",
    "            continue\n",
    "\n",
    "        # Definir limiar de \"mês epidêmico\" (percentil da série dessa doença)\n",
    "        limiar = y_cont_use.quantile(percentile_limiar)\n",
    "        print(f\"Limiar (p{int(percentile_limiar*100)}) para {target_col}: {limiar:.2f} casos\")\n",
    "\n",
    "        # y binário\n",
    "        y_bin = (y_cont_use >= limiar).astype(int)\n",
    "\n",
    "        # Agora, X definitivo: remove qualquer resto de coluna que não queremos\n",
    "        X = X_pre.copy()\n",
    "\n",
    "        # garantir que doenças não estão em X (por segurança extra)\n",
    "        X = X.drop(columns=[c for c in possible_disease_cols if c in X.columns], errors=\"ignore\")\n",
    "\n",
    "        # tirar Mes_do_ano (já temos sin/cos)\n",
    "        X = X.drop(columns=[\"Mes_do_ano\"], errors=\"ignore\")\n",
    "\n",
    "        # conferência de classes\n",
    "        n_pos = int(y_bin.sum())\n",
    "        n_total = len(y_bin)\n",
    "        print(f\"{target_col}: {n_pos} meses epidêmicos (1) de {n_total} meses totais \"\n",
    "              f\"({n_pos/n_total:.1%} positivos)\")\n",
    "\n",
    "        # SPLIT TEMPORAL (75% / 25%)\n",
    "        n = len(X)\n",
    "        split_idx = int(n * 0.75)\n",
    "\n",
    "        X_train, X_test = X.iloc[:split_idx], X.iloc[split_idx:]\n",
    "        y_train, y_test = y_bin.iloc[:split_idx], y_bin.iloc[split_idx:]\n",
    "\n",
    "        print(f\"Treino: {len(X_train)} meses, Teste: {len(X_test)} meses\")\n",
    "\n",
    "        # precisa de pelo menos 1 positivo e 1 negativo em treino e teste\n",
    "        if y_train.nunique() < 2 or y_test.nunique() < 2:\n",
    "            print(f\"[AVISO] Conjunto de treino ou teste de {target_col} só tem uma classe.\"\n",
    "                  f\" ROC/AUC ficam indefinidos. Pulando classificação.\")\n",
    "            continue\n",
    "\n",
    "        # Detectar tipos de colunas\n",
    "        numeric_features = X_train.select_dtypes(include=[np.number]).columns.tolist()\n",
    "        categorical_features = X_train.select_dtypes(exclude=[np.number]).columns.tolist()\n",
    "\n",
    "        preprocess = ColumnTransformer(\n",
    "            transformers=[\n",
    "                (\"num\", \"passthrough\", numeric_features),\n",
    "                (\"cat\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False), categorical_features),\n",
    "            ],\n",
    "            remainder=\"drop\"\n",
    "        )\n",
    "\n",
    "        # Random Forest Classifier \"domada\"\n",
    "        model = Pipeline(steps=[\n",
    "            (\"preprocess\", preprocess),\n",
    "            (\"rf\", RandomForestClassifier(\n",
    "                n_estimators=500,\n",
    "                max_depth=5,\n",
    "                min_samples_leaf=3,\n",
    "                max_features=\"sqrt\",\n",
    "                random_state=42,\n",
    "                n_jobs=-1\n",
    "            ))\n",
    "        ])\n",
    "\n",
    "        # treinar e prever\n",
    "        model.fit(X_train, y_train)\n",
    "        y_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "        # AUC geral (independe do threshold)\n",
    "        try:\n",
    "            auc = roc_auc_score(y_test, y_proba)\n",
    "        except ValueError:\n",
    "            auc = np.nan\n",
    "\n",
    "        # ROC curve\n",
    "        fpr, tpr, roc_thresholds = roc_curve(y_test, y_proba)\n",
    "\n",
    "        # Figura ROC\n",
    "        fig_roc, ax_roc = plt.subplots(figsize=(6, 5))\n",
    "        ax_roc.plot(fpr, tpr, label=f\"ROC (AUC = {auc:.3f})\")\n",
    "        ax_roc.plot([0, 1], [0, 1], linestyle=\"--\", label=\"Aleatório\")\n",
    "        ax_roc.set_xlabel(\"Falso Positivo (1 - Especificidade)\")\n",
    "        ax_roc.set_ylabel(\"Verdadeiro Positivo (Sensibilidade)\")\n",
    "        ax_roc.set_title(f\"Curva ROC — {target_col}\")\n",
    "        ax_roc.legend(loc=\"lower right\")\n",
    "        fig_roc.tight_layout()\n",
    "\n",
    "        roc_path = output_dir_class / f\"ROC_{target_col}.png\"\n",
    "        fig_roc.savefig(roc_path, dpi=300, bbox_inches=\"tight\")\n",
    "        plt.close(fig_roc)\n",
    "\n",
    "        # VARREDURA DE THRESHOLDS (0.1, 0.2, ..., 0.9)\n",
    "        best_thr = None\n",
    "        best_youden = -np.inf\n",
    "        best_metrics = None\n",
    "\n",
    "        for thr in thresholds_list:\n",
    "            y_pred_thr = (y_proba >= thr).astype(int)\n",
    "\n",
    "            acc = accuracy_score(y_test, y_pred_thr)\n",
    "            prec = precision_score(y_test, y_pred_thr, zero_division=0)\n",
    "            rec = recall_score(y_test, y_pred_thr, zero_division=0)\n",
    "            f1 = f1_score(y_test, y_pred_thr, zero_division=0)\n",
    "\n",
    "            # matriz de confusão pra especificidade\n",
    "            tn, fp, fn, tp = confusion_matrix(y_test, y_pred_thr, labels=[0, 1]).ravel()\n",
    "            sens = rec\n",
    "            espec = tn / (tn + fp) if (tn + fp) > 0 else np.nan\n",
    "\n",
    "            # Índice de Youden: sens + espec - 1\n",
    "            youden = sens + espec - 1 if np.isfinite(sens) and np.isfinite(espec) else -np.inf\n",
    "\n",
    "            thresholds_linhas.append({\n",
    "                \"doenca\": target_col,\n",
    "                \"threshold\": thr,\n",
    "                \"AUC\": auc,\n",
    "                \"accuracy\": acc,\n",
    "                \"precision\": prec,\n",
    "                \"recall_sensibilidade\": sens,\n",
    "                \"especificidade\": espec,\n",
    "                \"f1\": f1,\n",
    "                \"youden\": youden,\n",
    "                \"n_treino\": len(X_train),\n",
    "                \"n_teste\": len(X_test),\n",
    "                \"n_pos_treino\": int(y_train.sum()),\n",
    "                \"n_pos_teste\": int(y_test.sum()),\n",
    "                \"limiar_casos_p\": percentile_limiar,\n",
    "                \"valor_limiar_casos\": limiar\n",
    "            })\n",
    "\n",
    "            # Atualizar melhor threshold (maior índice de Youden)\n",
    "            if youden > best_youden:\n",
    "                best_youden = youden\n",
    "                best_thr = thr\n",
    "                best_metrics = {\n",
    "                    \"accuracy\": acc,\n",
    "                    \"precision\": prec,\n",
    "                    \"recall_sensibilidade\": sens,\n",
    "                    \"especificidade\": espec,\n",
    "                    \"f1\": f1\n",
    "                }\n",
    "\n",
    "        if best_thr is None:\n",
    "            print(f\"[AVISO] Não foi possível definir best_threshold para {target_col}. Pulando resumo.\")\n",
    "            continue\n",
    "\n",
    "        print(f\"Melhor limiar para {target_col} (Youden): {best_thr:.2f}\")\n",
    "        print(f\"  Sensibilidade: {best_metrics['recall_sensibilidade']:.3f}  \"\n",
    "              f\"Especificidade: {best_metrics['especificidade']:.3f}  \"\n",
    "              f\"Acurácia: {best_metrics['accuracy']:.3f}\")\n",
    "\n",
    "        # Matriz de confusão no melhor threshold\n",
    "        y_pred_best = (y_proba >= best_thr).astype(int)\n",
    "        cm = confusion_matrix(y_test, y_pred_best, labels=[0, 1])\n",
    "        cm_df = pd.DataFrame(\n",
    "            cm,\n",
    "            index=[\"Real 0 (não epidêmico)\", \"Real 1 (epidêmico)\"],\n",
    "            columns=[\"Previsto 0\", \"Previsto 1\"]\n",
    "        )\n",
    "\n",
    "        fig_cm, ax_cm = plt.subplots(figsize=(6, 5))\n",
    "        sns.heatmap(cm_df, annot=True, fmt=\"d\", ax=ax_cm)\n",
    "        ax_cm.set_title(f\"Matriz de Confusão — {target_col}\\nThreshold = {best_thr:.2f}\")\n",
    "        ax_cm.set_ylabel(\"Real\")\n",
    "        ax_cm.set_xlabel(\"Previsto\")\n",
    "        fig_cm.tight_layout()\n",
    "\n",
    "        cm_path = output_dir_class / f\"CM_{target_col}_best.png\"\n",
    "        fig_cm.savefig(cm_path, dpi=300, bbox_inches=\"tight\")\n",
    "        plt.close(fig_cm)\n",
    "\n",
    "        # Linha do resumo para essa doença\n",
    "        resumo_linhas.append({\n",
    "            \"doenca\": target_col,\n",
    "            \"AUC\": auc,\n",
    "            \"best_threshold\": best_thr,\n",
    "            \"accuracy_best\": best_metrics[\"accuracy\"],\n",
    "            \"precision_best\": best_metrics[\"precision\"],\n",
    "            \"recall_sensibilidade_best\": best_metrics[\"recall_sensibilidade\"],\n",
    "            \"especificidade_best\": best_metrics[\"especificidade\"],\n",
    "            \"f1_best\": best_metrics[\"f1\"],\n",
    "            \"n_treino\": len(X_train),\n",
    "            \"n_teste\": len(X_test),\n",
    "            \"n_pos_treino\": int(y_train.sum()),\n",
    "            \"n_pos_teste\": int(y_test.sum()),\n",
    "            \"limiar_casos_percentil\": percentile_limiar,\n",
    "            \"valor_limiar_casos\": limiar\n",
    "        })\n",
    "\n",
    "    # Construir DataFrames finais\n",
    "    resumo_df = pd.DataFrame(resumo_linhas)\n",
    "    thresholds_all_df = pd.DataFrame(thresholds_linhas)\n",
    "\n",
    "    return resumo_df, thresholds_all_df\n",
    "\n",
    "\n",
    "# =======================\n",
    "# Rodar Modelo 4 em df\n",
    "# =======================\n",
    "\n",
    "resumo_epidemico, thresholds_epidemico = classificar_meses_epidemicos(df)\n",
    "\n",
    "print(\"\\n=== RESUMO GERAL CLASSIFICAÇÃO EPIDÊMICO vs NÃO ===\")\n",
    "print(resumo_epidemico)\n",
    "\n",
    "\n",
    "# ==============================================\n",
    "# Salvar tabelas em Excel (uma aba por doença + Resumo)\n",
    "# ==============================================\n",
    "\n",
    "tabela_dir = base_path / \"RF_Modelo4_Classificacao_Epidemico\"\n",
    "tabela_dir.mkdir(exist_ok=True)\n",
    "\n",
    "excel_path = tabela_dir / \"Tabela_Modelo4_Classificacao_Epidemico.xlsx\"\n",
    "\n",
    "with pd.ExcelWriter(excel_path, engine=\"openpyxl\") as writer:\n",
    "    # aba Resumo\n",
    "    resumo_epidemico.to_excel(writer, sheet_name=\"Resumo\", index=False)\n",
    "\n",
    "    # abas por doença com tabela de thresholds\n",
    "    for doenca in possible_disease_cols:\n",
    "        df_thr_doenca = thresholds_epidemico[thresholds_epidemico[\"doenca\"] == doenca].copy()\n",
    "        if df_thr_doenca.empty:\n",
    "            continue\n",
    "\n",
    "        sheet_name = doenca[:31]\n",
    "        df_thr_doenca.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "\n",
    "print(f\"\\nArquivo Excel de classificação salvo em: {excel_path}\")\n",
    "print(f\"Figuras (ROC e Matriz de Confusão) salvas em: {output_dir_class}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a867a978",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
